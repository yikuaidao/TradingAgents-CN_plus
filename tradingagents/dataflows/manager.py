"""
Data source manager that orchestrates multiple adapters with priority and optional consistency checks
"""
from typing import List, Optional, Tuple, Dict, Any
import os
import logging
from datetime import datetime, timedelta
import pandas as pd
from pymongo import UpdateOne

from app.services.data_sources.base import DataSourceAdapter
from .providers.tushare.adapter import TushareAdapter
# Temporarily import legacy adapters from app until migrated
from app.services.data_sources.akshare_adapter import AKShareAdapter
from app.services.data_sources.baostock_adapter import BaoStockAdapter

logger = logging.getLogger(__name__)


class DataSourceManager:
    """
    Êï∞ÊçÆÊ∫êÁÆ°ÁêÜÂô®
    - ÁÆ°ÁêÜÂ§ö‰∏™ÈÄÇÈÖçÂô®ÔºåÂü∫‰∫é‰ºòÂÖàÁ∫ßÊéíÂ∫è
    - Êèê‰æõ fallback Ëé∑ÂèñËÉΩÂäõ
    - ÂèØÈÄâÔºö‰∏ÄËá¥ÊÄßÊ£ÄÊü•ÔºàËã•‰æùËµñÂ≠òÂú®Ôºâ
    - Âº∫Âà∂Êï∞ÊçÆËêΩÂ∫ì (Write-Through)
    """

    def __init__(self):
        self.adapters: List[DataSourceAdapter] = [
            TushareAdapter(),
            AKShareAdapter(),
            BaoStockAdapter(),
        ]

        # ‰ªéÊï∞ÊçÆÂ∫ìÂä†ËΩΩ‰ºòÂÖàÁ∫ßÈÖçÁΩÆ
        self._load_priority_from_database()

        # Êåâ‰ºòÂÖàÁ∫ßÊéíÂ∫èÔºàÊï∞Â≠óË∂äÂ§ß‰ºòÂÖàÁ∫ßË∂äÈ´òÔºåÊâÄ‰ª•ÈôçÂ∫èÊéíÂàóÔºâ
        self.adapters.sort(key=lambda x: x.priority, reverse=True)

        try:
            from app.services.data_sources.data_consistency_checker import DataConsistencyChecker
            self.consistency_checker = DataConsistencyChecker()
        except Exception:
            logger.warning("‚ö†Ô∏è Êï∞ÊçÆ‰∏ÄËá¥ÊÄßÊ£ÄÊü•Âô®‰∏çÂèØÁî®")
            self.consistency_checker = None

    def _load_priority_from_database(self):
        """‰ªéÊï∞ÊçÆÂ∫ìÂä†ËΩΩÊï∞ÊçÆÊ∫ê‰ºòÂÖàÁ∫ßÈÖçÁΩÆÔºà‰ªé datasource_groupings ÈõÜÂêàËØªÂèñ AËÇ°Â∏ÇÂú∫ÁöÑ‰ºòÂÖàÁ∫ßÔºâ"""
        try:
            from app.core.database import get_mongo_db_sync
            db = get_mongo_db_sync()
            groupings_collection = db.datasource_groupings

            # Êü•ËØ¢ AËÇ°Â∏ÇÂú∫ÁöÑÊï∞ÊçÆÊ∫êÂàÜÁªÑÈÖçÁΩÆ
            groupings = list(groupings_collection.find({
                "market_category_id": "a_shares",
                "enabled": True
            }))

            if groupings:
                # ÂàõÂª∫ÂêçÁß∞Âà∞‰ºòÂÖàÁ∫ßÁöÑÊò†Â∞ÑÔºàÊï∞ÊçÆÊ∫êÂêçÁß∞ÈúÄË¶ÅËΩ¨Êç¢‰∏∫Â∞èÂÜôÔºâ
                priority_map = {}
                for grouping in groupings:
                    data_source_name = grouping.get('data_source_name', '').lower()
                    priority = grouping.get('priority')
                    if data_source_name and priority is not None:
                        priority_map[data_source_name] = priority
                        logger.info(f"üìä ‰ªéÊï∞ÊçÆÂ∫ìËØªÂèñ {data_source_name} Âú® AËÇ°Â∏ÇÂú∫ÁöÑ‰ºòÂÖàÁ∫ß: {priority}")

                # Êõ¥Êñ∞ÂêÑ‰∏™ Adapter ÁöÑ‰ºòÂÖàÁ∫ß
                for adapter in self.adapters:
                    if adapter.name in priority_map:
                        # Âä®ÊÄÅËÆæÁΩÆ‰ºòÂÖàÁ∫ß
                        adapter._priority = priority_map[adapter.name]
                        logger.info(f"‚úÖ ËÆæÁΩÆ {adapter.name} ‰ºòÂÖàÁ∫ß: {adapter._priority}")
                    else:
                        # ‰ΩøÁî®ÈªòËÆ§‰ºòÂÖàÁ∫ß
                        adapter._priority = adapter._get_default_priority()
                        logger.info(f"‚ö†Ô∏è Êï∞ÊçÆÂ∫ì‰∏≠Êú™ÊâæÂà∞ {adapter.name} ÈÖçÁΩÆÔºå‰ΩøÁî®ÈªòËÆ§‰ºòÂÖàÁ∫ß: {adapter._priority}")
            else:
                logger.info("‚ö†Ô∏è Êï∞ÊçÆÂ∫ì‰∏≠Êú™ÊâæÂà∞ AËÇ°Â∏ÇÂú∫ÁöÑÊï∞ÊçÆÊ∫êÈÖçÁΩÆÔºåÂ∞ùËØï‰ΩøÁî®ÁéØÂ¢ÉÂèòÈáèÊàñÈªòËÆ§‰ºòÂÖàÁ∫ß")
                self._apply_env_or_default_priority()
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è ‰ªéÊï∞ÊçÆÂ∫ìÂä†ËΩΩ‰ºòÂÖàÁ∫ßÂ§±Ë¥•: {e}ÔºåÂ∞ùËØï‰ΩøÁî®ÁéØÂ¢ÉÂèòÈáèÊàñÈªòËÆ§‰ºòÂÖàÁ∫ß")
            # import traceback
            # logger.warning(f"Â†ÜÊ†àË∑üË∏™:\n{traceback.format_exc()}")
            self._apply_env_or_default_priority()

    def _apply_env_or_default_priority(self):
        """Â∫îÁî®ÁéØÂ¢ÉÂèòÈáèÊàñÈªòËÆ§‰ºòÂÖàÁ∫ß"""
        # 1. ÈáçÁΩÆ‰∏∫ÈªòËÆ§‰ºòÂÖàÁ∫ß
        for adapter in self.adapters:
            adapter._priority = adapter._get_default_priority()

        # 2. Ê£ÄÊü•ÁéØÂ¢ÉÂèòÈáè
        default_source = os.getenv("DEFAULT_CHINA_DATA_SOURCE", "").lower()
        if default_source:
            logger.info(f"üîß Ê£ÄÊµãÂà∞ÁéØÂ¢ÉÂèòÈáè DEFAULT_CHINA_DATA_SOURCE={default_source}")
            target_adapter = next((a for a in self.adapters if a.name == default_source), None)
            if target_adapter:
                # ÊèêÂçá‰ºòÂÖàÁ∫ßÔºå‰ΩøÂÖ∂È´ò‰∫éÈªòËÆ§ÂÄº (Tushare=3)
                target_adapter._priority = 10
                logger.info(f"‚úÖ Â∞Ü {target_adapter.name} ‰ºòÂÖàÁ∫ßÊèêÂçáËá≥ 10 (Âü∫‰∫éÁéØÂ¢ÉÂèòÈáè)")

    def get_available_adapters(self) -> List[DataSourceAdapter]:
        available: List[DataSourceAdapter] = []
        for adapter in self.adapters:
            if adapter.is_available():
                available.append(adapter)
                logger.info(
                    f"Data source {adapter.name} is available (priority: {adapter.priority})"
                )
            else:
                logger.warning(f"Data source {adapter.name} is not available")
        return available

    def _save_kline_to_db(self, code: str, items: List[Dict], source: str, period: str):
        """
        ÂêåÊ≠•Â∞Ü K Á∫øÊï∞ÊçÆÂÜôÂÖ• MongoDB (Write-Through)
        Âü∫‰∫é stock_daily_quotes ÈõÜÂêàÁªìÊûÑ
        """
        try:
            from app.core.database import get_mongo_db_sync
            db = get_mongo_db_sync()
            collection = db.stock_daily_quotes

            # ÂáÜÂ§áÊâπÈáèÂÜôÂÖ•Êìç‰Ωú
            operations = []

            # Ê†áÂáÜÂåñÂ§ÑÁêÜ
            # ÂÅáËÆæ items ÊòØ [{time, open, high, low, close, volume, amount}, ...]
            # ÈúÄË¶ÅËΩ¨Êç¢‰∏∫ stock_daily_quotes ÁöÑÂ≠óÊÆµ: symbol, trade_date, data_source, period, open, high, low, close, volume, amount

            # ÁßªÈô§ .SZ/.SH ÂêéÁºÄËé∑ÂèñÁ∫ØÊï∞Â≠ó‰ª£Á†Å
            symbol = code.split('.')[0]

            for item in items:
                trade_date = str(item.get('time', '')).replace('-', '').replace('/', '')
                if not trade_date:
                    continue

                doc = {
                    "symbol": symbol,
                    "trade_date": trade_date,
                    "data_source": source,
                    "period": period,
                    "market": "CN", # ÈªòËÆ§‰∏∫ CNÔºåÂêéÁª≠ÂèØÊâ©Â±ï
                    "open": item.get('open'),
                    "high": item.get('high'),
                    "low": item.get('low'),
                    "close": item.get('close'),
                    "volume": item.get('volume'),
                    "amount": item.get('amount'),
                    "updated_at": datetime.utcnow()
                }

                # ÊûÑÂª∫ÂîØ‰∏ÄÁ¥¢ÂºïÊü•ËØ¢Êù°‰ª∂
                filter_query = {
                    "symbol": symbol,
                    "trade_date": trade_date,
                    "data_source": source,
                    "period": period
                }

                operations.append(
                    UpdateOne(filter_query, {"$set": doc}, upsert=True)
                )

            if operations:
                result = collection.bulk_write(operations, ordered=False)
                logger.info(f"üíæ [Write-Through] Saved {len(operations)} records for {code} to DB (Upserted: {result.upserted_count}, Modified: {result.modified_count})")

        except Exception as e:
            logger.error(f"‚ùå Failed to save kline to DB for {code}: {e}")

    def sync_stock_data(self, codes: List[str], period: str = "day"):
        """
        ÊâπÈáèÂêåÊ≠•ËÇ°Á•®Êï∞ÊçÆ (Pre-Inference Sync)
        Âº∫Âà∂‰ªéÊé•Âè£ÊãâÂèñÂπ∂ÂÖ•Â∫ì
        """
        logger.info(f"üîÑ Starting batch sync for {len(codes)} stocks...")
        success_count = 0
        for code in codes:
            try:
                # Ë∞ÉÁî® get_kline_with_fallback ‰ºöËá™Âä®Ëß¶Âèë Write-Through
                items, source = self.get_kline_with_fallback(code, period=period)
                if items:
                    success_count += 1
            except Exception as e:
                logger.error(f"Failed to sync {code}: {e}")
        logger.info(f"‚úÖ Batch sync completed. Success: {success_count}/{len(codes)}")

    def get_stock_list_with_fallback(self, preferred_sources: Optional[List[str]] = None) -> Tuple[Optional[pd.DataFrame], Optional[str]]:
        """
        Ëé∑ÂèñËÇ°Á•®ÂàóË°®ÔºåÊîØÊåÅÊåáÂÆö‰ºòÂÖàÊï∞ÊçÆÊ∫ê
        """
        available_adapters = self.get_available_adapters()

        if preferred_sources:
            logger.info(f"Using preferred data sources: {preferred_sources}")
            priority_map = {name: idx for idx, name in enumerate(preferred_sources)}
            preferred = [a for a in available_adapters if a.name in priority_map]
            others = [a for a in available_adapters if a.name not in priority_map]
            preferred.sort(key=lambda a: priority_map.get(a.name, 999))
            available_adapters = preferred + others
            logger.info(f"Reordered adapters: {[a.name for a in available_adapters]}")

        for adapter in available_adapters:
            try:
                logger.info(f"Trying to fetch stock list from {adapter.name}")
                df = adapter.get_stock_list()
                if df is not None and not df.empty:
                    return df, adapter.name
            except Exception as e:
                logger.error(f"Failed to fetch stock list from {adapter.name}: {e}")
                continue
        return None, None

    def get_daily_basic_with_fallback(self, trade_date: str, preferred_sources: Optional[List[str]] = None) -> Tuple[Optional[pd.DataFrame], Optional[str]]:
        """
        Ëé∑ÂèñÊØèÊó•Âü∫Á°ÄÊï∞ÊçÆÔºåÊîØÊåÅÊåáÂÆö‰ºòÂÖàÊï∞ÊçÆÊ∫ê
        """
        available_adapters = self.get_available_adapters()

        if preferred_sources:
            priority_map = {name: idx for idx, name in enumerate(preferred_sources)}
            preferred = [a for a in available_adapters if a.name in priority_map]
            others = [a for a in available_adapters if a.name not in priority_map]
            preferred.sort(key=lambda a: priority_map.get(a.name, 999))
            available_adapters = preferred + others

        for adapter in available_adapters:
            try:
                logger.info(f"Trying to fetch daily basic data from {adapter.name}")
                df = adapter.get_daily_basic(trade_date)
                if df is not None and not df.empty:
                    return df, adapter.name
            except Exception as e:
                logger.error(f"Failed to fetch daily basic data from {adapter.name}: {e}")
                continue
        return None, None

    def find_latest_trade_date_with_fallback(self, preferred_sources: Optional[List[str]] = None) -> Optional[str]:
        """
        Êü•ÊâæÊúÄÊñ∞‰∫§ÊòìÊó•ÊúüÔºåÊîØÊåÅÊåáÂÆö‰ºòÂÖàÊï∞ÊçÆÊ∫ê
        """
        available_adapters = self.get_available_adapters()

        if preferred_sources:
            priority_map = {name: idx for idx, name in enumerate(preferred_sources)}
            preferred = [a for a in available_adapters if a.name in priority_map]
            others = [a for a in available_adapters if a.name not in priority_map]
            preferred.sort(key=lambda a: priority_map.get(a.name, 999))
            available_adapters = preferred + others

        for adapter in available_adapters:
            try:
                trade_date = adapter.find_latest_trade_date()
                if trade_date:
                    return trade_date
            except Exception as e:
                logger.error(f"Failed to find trade date from {adapter.name}: {e}")
                continue
        return (datetime.now() - timedelta(days=1)).strftime("%Y%m%d")

    def get_realtime_quotes_with_fallback(self) -> Tuple[Optional[Dict], Optional[str]]:
        """
        Ëé∑ÂèñÂÖ®Â∏ÇÂú∫ÂÆûÊó∂Âø´ÁÖßÔºåÊåâÈÄÇÈÖçÂô®‰ºòÂÖàÁ∫ß‰æùÊ¨°Â∞ùËØïÔºåËøîÂõûÈ¶ñ‰∏™ÊàêÂäüÁªìÊûú
        Returns: (quotes_dict, source_name)
        quotes_dict ÂΩ¢Â¶Ç { '000001': {'close': 10.0, 'pct_chg': 1.2, 'amount': 1.2e8}, ... }
        """
        available_adapters = self.get_available_adapters()
        for adapter in available_adapters:
            try:
                logger.info(f"Trying to fetch realtime quotes from {adapter.name}")
                data = adapter.get_realtime_quotes()
                if data:
                    return data, adapter.name
            except Exception as e:
                logger.error(f"Failed to fetch realtime quotes from {adapter.name}: {e}")
                continue
        return None, None


    def get_daily_basic_with_consistency_check(
        self, trade_date: str
    ) -> Tuple[Optional[pd.DataFrame], Optional[str], Optional[Dict]]:
        """
        ‰ΩøÁî®‰∏ÄËá¥ÊÄßÊ£ÄÊü•Ëé∑ÂèñÊØèÊó•Âü∫Á°ÄÊï∞ÊçÆ
        """
        available_adapters = self.get_available_adapters()
        if len(available_adapters) < 2:
            df, source = self.get_daily_basic_with_fallback(trade_date)
            return df, source, None
        primary_adapter = available_adapters[0]
        secondary_adapter = available_adapters[1]
        try:
            logger.info(
                f"üîç Ëé∑ÂèñÊï∞ÊçÆËøõË°å‰∏ÄËá¥ÊÄßÊ£ÄÊü•: {primary_adapter.name} vs {secondary_adapter.name}"
            )
            primary_data = primary_adapter.get_daily_basic(trade_date)
            secondary_data = secondary_adapter.get_daily_basic(trade_date)
            if primary_data is None or primary_data.empty:
                logger.warning(f"‚ö†Ô∏è ‰∏ªÊï∞ÊçÆÊ∫ê{primary_adapter.name}Â§±Ë¥•Ôºå‰ΩøÁî®fallback")
                df, source = self.get_daily_basic_with_fallback(trade_date)
                return df, source, None
            if secondary_data is None or secondary_data.empty:
                logger.warning(f"‚ö†Ô∏è Ê¨°Êï∞ÊçÆÊ∫ê{secondary_adapter.name}Â§±Ë¥•Ôºå‰ΩøÁî®‰∏ªÊï∞ÊçÆÊ∫ê")
                return primary_data, primary_adapter.name, None
            if self.consistency_checker:
                consistency_result = self.consistency_checker.check_daily_basic_consistency(
                    primary_data,
                    secondary_data,
                    primary_adapter.name,
                    secondary_adapter.name,
                )
                final_data, resolution_strategy = self.consistency_checker.resolve_data_conflicts(
                    primary_data, secondary_data, consistency_result
                )
                consistency_report = {
                    'is_consistent': consistency_result.is_consistent,
                    'confidence_score': consistency_result.confidence_score,
                    'recommended_action': consistency_result.recommended_action,
                    'resolution_strategy': resolution_strategy,
                    'differences': consistency_result.differences,
                    'primary_source': primary_adapter.name,
                    'secondary_source': secondary_adapter.name,
                }
                logger.info(
                    f"üìä Êï∞ÊçÆ‰∏ÄËá¥ÊÄßÊ£ÄÊü•ÂÆåÊàê: ÁΩÆ‰ø°Â∫¶={consistency_result.confidence_score:.2f}, Á≠ñÁï•={consistency_result.recommended_action}"
                )
                return final_data, primary_adapter.name, consistency_report
            else:
                logger.warning("‚ö†Ô∏è ‰∏ÄËá¥ÊÄßÊ£ÄÊü•Âô®‰∏çÂèØÁî®Ôºå‰ΩøÁî®‰∏ªÊï∞ÊçÆÊ∫ê")
                return primary_data, primary_adapter.name, None
        except Exception as e:
            logger.error(f"‚ùå ‰∏ÄËá¥ÊÄßÊ£ÄÊü•Â§±Ë¥•: {e}")
            df, source = self.get_daily_basic_with_fallback(trade_date)
            return df, source, None


    def get_kline_with_fallback(self, code: str, period: str = "day", limit: int = 120, adj: Optional[str] = None) -> Tuple[Optional[List[Dict]], Optional[str]]:
        """Êåâ‰ºòÂÖàÁ∫ßÂ∞ùËØïËé∑ÂèñKÁ∫øÔºåËøîÂõû(items, source)"""
        available_adapters = self.get_available_adapters()
        for adapter in available_adapters:
            try:
                logger.info(f"Trying to fetch kline from {adapter.name}")
                items = adapter.get_kline(code=code, period=period, limit=limit, adj=adj)
                if items:
                    # üî• Write-Through: Á´ãÂç≥ÂÜôÂÖ•Êï∞ÊçÆÂ∫ì
                    self._save_kline_to_db(code, items, adapter.name, period)
                    return items, adapter.name
            except Exception as e:
                logger.error(f"Failed to fetch kline from {adapter.name}: {e}")
                continue
        return None, None

    def get_kline_all_sources(self, code: str, period: str = "day", limit: int = 120, adj: Optional[str] = None) -> Dict[str, List[Dict]]:
        """Ëé∑ÂèñÊâÄÊúâÂèØÁî®Êï∞ÊçÆÊ∫êÁöÑKÁ∫øÊï∞ÊçÆ"""
        available_adapters = self.get_available_adapters()
        results = {}
        for adapter in available_adapters:
            try:
                logger.info(f"Trying to fetch kline from {adapter.name}")
                items = adapter.get_kline(code=code, period=period, limit=limit, adj=adj)
                if items:
                    results[adapter.name] = items
                    # üî• Write-Through: Á´ãÂç≥ÂÜôÂÖ•Êï∞ÊçÆÂ∫ì
                    self._save_kline_to_db(code, items, adapter.name, period)
            except Exception as e:
                logger.error(f"Failed to fetch kline from {adapter.name}: {e}")
                continue
        return results

    def get_news_with_fallback(self, code: str, days: int = 2, limit: int = 50, include_announcements: bool = True) -> Tuple[Optional[List[Dict]], Optional[str]]:
        """Êåâ‰ºòÂÖàÁ∫ßÂ∞ùËØïËé∑ÂèñÊñ∞Èóª‰∏éÂÖ¨ÂëäÔºåËøîÂõû(items, source)"""
        available_adapters = self.get_available_adapters()
        for adapter in available_adapters:
            try:
                logger.info(f"Trying to fetch news from {adapter.name}")
                items = adapter.get_news(code=code, days=days, limit=limit, include_announcements=include_announcements)
                if items:
                    return items, adapter.name
            except Exception as e:
                logger.error(f"Failed to fetch news from {adapter.name}: {e}")
                continue
        return None, None

    def _query_with_fallback(self, api_name: str, **kwargs) -> Optional[Any]:
        """Generic query with fallback"""
        available_adapters = self.get_available_adapters()
        for adapter in available_adapters:
            try:
                # logger.info(f"Trying to query {api_name} from {adapter.name}")
                if hasattr(adapter, "query"):
                    df = adapter.query(api_name, **kwargs)
                    if df is not None and not df.empty:
                        return df.to_dict(orient="records")
            except Exception as e:
                logger.error(f"Query {api_name} from {adapter.name} failed: {e}")
                continue
        return None

    # --- Extended Finance Tools Implementation ---

    def get_stock_data(self, code: str, market_type: str, start_date: str, end_date: str, indicators: str = None):
        # Implementation for stock data (similar to get_kline but with market type and formatting)
        # For now, reuse get_kline_with_fallback if market is CN
        if market_type == "cn":
            items, _ = self.get_kline_with_fallback(code, period="day", limit=300) # approximate limit
            # Note: start/end date filtering needs to be applied if adapter doesn't support it directly
            # Tushare adapter pro_bar supports start_date/end_date but get_kline interface uses limit.
            # I should update get_kline interface or use query.
            # Using query is more flexible.
            return self._query_with_fallback("daily", ts_code=code, start_date=start_date, end_date=end_date)
        elif market_type == "hk":
            return self._query_with_fallback("hk_daily", ts_code=code, start_date=start_date, end_date=end_date)
        elif market_type == "us":
            return self._query_with_fallback("us_daily", ts_code=code, start_date=start_date, end_date=end_date)
        return None

    def get_stock_data_minutes(self, market_type: str, code: str, start_datetime: str, end_datetime: str, freq: str):
        # freq mapping: 1min, 5min...
        return self._query_with_fallback("stk_mins", ts_code=code, start_date=start_datetime, end_date=end_datetime, freq=freq)

    def get_company_performance(self, ts_code: str, data_type: str, start_date: str, end_date: str, period: str = None, ind_name: str = None, market: str = "cn"):
        api_map = {
            "cn": {
                "forecast": "forecast", "express": "express", "indicators": "fina_indicator",
                "dividend": "dividend", "mainbz": "fina_mainbz", "holder_number": "stk_holdernumber",
                "holder_trade": "stk_holdertrade", "managers": "stk_managers", "audit": "fina_audit",
                "company_basic": "stock_company", "balance_basic": "balancesheet", "balance_all": "balancesheet",
                "cashflow_basic": "cashflow", "cashflow_all": "cashflow", "income_basic": "income", "income_all": "income",
                "share_float": "share_float", "repurchase": "repurchase", "top10_holders": "top10_holders",
                "top10_floatholders": "top10_floatholders", "pledge_stat": "pledge_stat", "pledge_detail": "pledge_detail"
            },
            "hk": {
                "income": "hk_income", "balance": "hk_balancesheet", "cashflow": "hk_cashflow"
            },
            "us": {
                "income": "us_income", "balance": "us_balancesheet", "cashflow": "us_cashflow", "indicator": "us_fina_indicator"
            }
        }
        api_name = api_map.get(market, {}).get(data_type)
        if not api_name:
            return None

        params = {"ts_code": ts_code, "start_date": start_date, "end_date": end_date}
        if period: params["period"] = period
        if ind_name: params["ind_name"] = ind_name

        return self._query_with_fallback(api_name, **params)

    def get_macro_econ(self, indicator: str, start_date: str, end_date: str):
        api_map = {
            "shibor": "shibor", "lpr": "lpr_data", "gdp": "cn_gdp", "cpi": "cn_cpi",
            "ppi": "cn_ppi", "cn_m": "cn_m", "cn_pmi": "cn_pmi", "cn_sf": "cn_sf",
            "shibor_quote": "shibor_quote", "libor": "libor", "hibor": "hibor"
        }
        api_name = api_map.get(indicator)
        if not api_name: return None
        return self._query_with_fallback(api_name, start_date=start_date, end_date=end_date)

    def get_money_flow(self, start_date: str, end_date: str, query_type: str = None, ts_code: str = None, content_type: str = None, trade_date: str = None):
        if not query_type:
            # Auto detect
            if ts_code and ts_code[0].isdigit(): query_type = "stock"
            else: query_type = "market"

        api_map = {"stock": "moneyflow_dc", "market": "moneyflow_mkt_dc", "sector": "moneyflow_ind_dc"}
        api_name = api_map.get(query_type, "moneyflow_dc")

        params = {"start_date": start_date, "end_date": end_date}
        if ts_code: params["ts_code"] = ts_code
        if trade_date: params["trade_date"] = trade_date

        return self._query_with_fallback(api_name, **params)

    def get_margin_trade(self, data_type: str, start_date: str, end_date: str = None, ts_code: str = None, exchange: str = None):
        # margin_secs, margin, margin_detail, slb_len_mm
        return self._query_with_fallback(data_type, start_date=start_date, end_date=end_date, ts_code=ts_code, exchange_id=exchange)

    def get_fund_data(self, ts_code: str, data_type: str, start_date: str = None, end_date: str = None, period: str = None):
        api_map = {
            "basic": "fund_basic", "manager": "fund_manager", "nav": "fund_nav",
            "dividend": "fund_div", "portfolio": "fund_portfolio"
        }
        api_name = api_map.get(data_type)
        if not api_name: return None

        params = {"ts_code": ts_code}
        if start_date: params["start_date"] = start_date
        if end_date: params["end_date"] = end_date
        if period: params["period"] = period

        return self._query_with_fallback(api_name, **params)

    def get_fund_manager_by_name(self, name: str, ann_date: str = None):
        params = {"name": name}
        if ann_date: params["ann_date"] = ann_date
        return self._query_with_fallback("fund_manager", **params)

    def get_index_data(self, code: str, start_date: str, end_date: str):
        return self._query_with_fallback("index_daily", ts_code=code, start_date=start_date, end_date=end_date)

    def get_csi_index_constituents(self, index_code: str, start_date: str, end_date: str):
        # This requires multiple calls (weight, daily, basic), simplifying to weight for now or index_weight
        return self._query_with_fallback("index_weight", index_code=index_code, start_date=start_date, end_date=end_date)

    def get_convertible_bond(self, data_type: str, ts_code: str = None, start_date: str = None, end_date: str = None):
        api_map = {"info": "cb_basic", "issue": "cb_issue"}
        api_name = api_map.get(data_type, "cb_basic")
        params = {}
        if ts_code: params["ts_code"] = ts_code
        if start_date: params["start_date"] = start_date
        if end_date: params["end_date"] = end_date
        return self._query_with_fallback(api_name, **params)

    def get_block_trade(self, start_date: str, end_date: str, code: str = None):
        params = {"start_date": start_date, "end_date": end_date}
        if code: params["ts_code"] = code
        return self._query_with_fallback("block_trade", **params)

    def get_dragon_tiger_inst(self, trade_date: str, ts_code: str = None):
        params = {"trade_date": trade_date}
        if ts_code: params["ts_code"] = ts_code
        return self._query_with_fallback("top_inst", **params)

    def get_finance_news(self, query: str):
        # Tushare doesn't have a search news by query API easily (news is stream).
        # We can try 'news' with src='sina' etc.
        # But 'finance_news' tool description says "Baidu News Crawler (Non-Tushare)".
        # Since I am in DataSourceManager, I should stick to adapters.
        # Tushare has `news` (major news) and `major_news` (CCTV).
        # I will use `news` for now.
        return self._query_with_fallback("news", src="sina", query=query) # Defaulting to Sina news stream

    def get_hot_news_7x24(self, limit: int = 100):
        # Tushare 'news' interface needs src, start_date, end_date.
        # But 'major_news' or 'cctv_news' might be better for 7x24.
        # Actually 'news' with src='sina' is often used for 7x24 rolling news.
        now = datetime.now()
        start = (now - timedelta(days=1)).strftime("%Y-%m-%d %H:%M:%S")
        end = now.strftime("%Y-%m-%d %H:%M:%S")
        return self._query_with_fallback("news", src="sina", start_date=start, end_date=end, limit=limit)
