# TradingAgents/graph/trading_graph.py

import os
from pathlib import Path
import json
from datetime import date
from typing import Dict, Any, Tuple, List, Optional
import time

from langchain_openai import ChatOpenAI
from langchain_anthropic import ChatAnthropic
from langchain_google_genai import ChatGoogleGenerativeAI
from tradingagents.llm_adapters import ChatDashScopeOpenAI, ChatGoogleOpenAI

from langgraph.prebuilt import ToolNode

from tradingagents.agents import *
from tradingagents.default_config import DEFAULT_CONFIG
from tradingagents.agents.utils.memory import FinancialSituationMemory

from langchain_core.tools import StructuredTool, BaseTool

# å¯¼å…¥ç»Ÿä¸€æ—¥å¿—ç³»ç»Ÿ
from tradingagents.utils.logging_init import get_logger

# å¯¼å…¥æ—¥å¿—æ¨¡å—
from tradingagents.utils.logging_manager import get_logger
from tradingagents.utils.runtime_paths import get_eval_results_dir
logger = get_logger('agents')
from tradingagents.agents.utils.agent_states import (
    AgentState,
    InvestDebateState,
    RiskDebateState,
)
from tradingagents.dataflows.interface import set_config

from .conditional_logic import ConditionalLogic
from .setup import GraphSetup
from .propagation import Propagator
from .reflection import Reflector
from .signal_processing import SignalProcessor


def create_llm_by_provider(provider: str, model: str, backend_url: str, temperature: float, max_tokens: int, timeout: int, api_key: str = None):
    """
    æ ¹æ® provider åˆ›å»ºå¯¹åº”çš„ LLM å®žä¾‹

    Args:
        provider: ä¾›åº”å•†åç§° (google, dashscope, deepseek, openai, etc.)
        model: æ¨¡åž‹åç§°
        backend_url: API åœ°å€
        temperature: æ¸©åº¦å‚æ•°
        max_tokens: æœ€å¤§ token æ•°
        timeout: è¶…æ—¶æ—¶é—´
        api_key: API Keyï¼ˆå¯é€‰ï¼Œå¦‚æžœæœªæä¾›åˆ™ä»ŽçŽ¯å¢ƒå˜é‡è¯»å–ï¼‰

    Returns:
        LLM å®žä¾‹
    """
    from tradingagents.llm_adapters.deepseek_adapter import ChatDeepSeek
    from tradingagents.llm_adapters.openai_compatible_base import create_openai_compatible_llm

    logger.info(f"ðŸ”§ [åˆ›å»ºLLM] provider={provider}, model={model}, url={backend_url}")
    logger.info(f"ðŸ”‘ [API Key] æ¥æº: {'æ•°æ®åº“é…ç½®' if api_key else 'çŽ¯å¢ƒå˜é‡'}")

    if provider.lower() == "google":
        # ä¼˜å…ˆä½¿ç”¨ä¼ å…¥çš„ API Keyï¼Œå¦åˆ™ä»ŽçŽ¯å¢ƒå˜é‡è¯»å–
        google_api_key = api_key or os.getenv('GOOGLE_API_KEY')
        if not google_api_key:
            raise ValueError("ä½¿ç”¨Googleéœ€è¦è®¾ç½®GOOGLE_API_KEYçŽ¯å¢ƒå˜é‡æˆ–åœ¨æ•°æ®åº“ä¸­é…ç½®API Key")

        # ä¼ é€’ base_url å‚æ•°ï¼Œä½¿åŽ‚å®¶é…ç½®çš„ default_base_url ç”Ÿæ•ˆ
        return ChatGoogleOpenAI(
            model=model,
            google_api_key=google_api_key,
            base_url=backend_url if backend_url else None,
            temperature=temperature,
            max_tokens=max_tokens,
            timeout=timeout
        )

    elif provider.lower() == "dashscope":
        # ä¼˜å…ˆä½¿ç”¨ä¼ å…¥çš„ API Keyï¼Œå¦åˆ™ä»ŽçŽ¯å¢ƒå˜é‡è¯»å–
        dashscope_api_key = api_key or os.getenv('DASHSCOPE_API_KEY')

        # ðŸ›¡ï¸ ç‰¹æ®Šå¤„ç†ï¼šå¦‚æžœç”¨æˆ·é…ç½®äº† native API URLï¼Œè‡ªåŠ¨ä¿®æ­£ä¸º None ä»¥ä½¿ç”¨é»˜è®¤çš„å…¼å®¹ URL
        # DashScope Native API: https://dashscope.aliyuncs.com/api/v1
        # OpenAI Compatible: https://dashscope.aliyuncs.com/compatible-mode/v1
        final_base_url = backend_url
        if backend_url and "dashscope.aliyuncs.com/api/v1" in backend_url:
            logger.warning(f"âš ï¸ [Config Correction] æ£€æµ‹åˆ° DashScope Native API URL ({backend_url})ï¼Œå·²è‡ªåŠ¨åˆ‡æ¢ä¸º OpenAI å…¼å®¹æ¨¡å¼ URL")
            final_base_url = None

        # ä¼ é€’ base_url å‚æ•°ï¼Œä½¿åŽ‚å®¶é…ç½®çš„ default_base_url ç”Ÿæ•ˆ
        return ChatDashScopeOpenAI(
            model=model,
            api_key=dashscope_api_key,  # ðŸ”¥ ä¼ é€’ API Key
            base_url=final_base_url if final_base_url else None,  # å¦‚æžœæœ‰è‡ªå®šä¹‰ URL åˆ™ä½¿ç”¨
            temperature=temperature,
            max_tokens=max_tokens,
            request_timeout=timeout
        )

    elif provider.lower() == "deepseek":
        # ä¼˜å…ˆä½¿ç”¨ä¼ å…¥çš„ API Keyï¼Œå¦åˆ™ä»ŽçŽ¯å¢ƒå˜é‡è¯»å–
        deepseek_api_key = api_key or os.getenv('DEEPSEEK_API_KEY')
        if not deepseek_api_key:
            raise ValueError("ä½¿ç”¨DeepSeekéœ€è¦è®¾ç½®DEEPSEEK_API_KEYçŽ¯å¢ƒå˜é‡æˆ–åœ¨æ•°æ®åº“ä¸­é…ç½®API Key")

        return ChatDeepSeek(
            model=model,
            api_key=deepseek_api_key,
            base_url=backend_url,
            temperature=temperature,
            max_tokens=max_tokens,
            timeout=timeout
        )

    elif provider.lower() == "zhipu":
        # æ™ºè°±AIå¤„ç†
        zhipu_api_key = api_key or os.getenv('ZHIPU_API_KEY')
        if not zhipu_api_key:
            raise ValueError("ä½¿ç”¨æ™ºè°±AIéœ€è¦è®¾ç½®ZHIPU_API_KEYçŽ¯å¢ƒå˜é‡æˆ–åœ¨æ•°æ®åº“ä¸­é…ç½®API Key")
        
        return create_openai_compatible_llm(
            provider="zhipu",
            model=model,
            api_key=zhipu_api_key,
            base_url=backend_url,  # ä½¿ç”¨ç”¨æˆ·æä¾›çš„backend_url
            temperature=temperature,
            max_tokens=max_tokens,
            timeout=timeout
        )

    elif provider.lower() in ["openai", "siliconflow", "openrouter", "ollama"]:
        # ä¼˜å…ˆä½¿ç”¨ä¼ å…¥çš„ API Keyï¼Œå¦åˆ™ä»ŽçŽ¯å¢ƒå˜é‡è¯»å–
        if not api_key:
            if provider.lower() == "siliconflow":
                api_key = os.getenv('SILICONFLOW_API_KEY')
            elif provider.lower() == "openrouter":
                api_key = os.getenv('OPENROUTER_API_KEY') or os.getenv('OPENAI_API_KEY')
            elif provider.lower() == "openai":
                api_key = os.getenv('OPENAI_API_KEY')

        return ChatOpenAI(
            model=model,
            base_url=backend_url,
            api_key=api_key,
            temperature=temperature,
            max_tokens=max_tokens,
            timeout=timeout
        )

    elif provider.lower() == "anthropic":
        return ChatAnthropic(
            model=model,
            base_url=backend_url,
            temperature=temperature,
            max_tokens=max_tokens,
            timeout=timeout
        )

    elif provider.lower() in ["qianfan", "custom_openai"]:
        return create_openai_compatible_llm(
            provider=provider,
            model=model,
            base_url=backend_url,
            temperature=temperature,
            max_tokens=max_tokens,
            timeout=timeout
        )

    else:
        # ðŸ”§ è‡ªå®šä¹‰åŽ‚å®¶ï¼šä½¿ç”¨ OpenAI å…¼å®¹æ¨¡å¼
        logger.info(f"ðŸ”§ ä½¿ç”¨ OpenAI å…¼å®¹æ¨¡å¼å¤„ç†è‡ªå®šä¹‰åŽ‚å®¶: {provider}")

        # å°è¯•ä»ŽçŽ¯å¢ƒå˜é‡èŽ·å– API Keyï¼ˆæ”¯æŒå¤šç§å‘½åæ ¼å¼ï¼‰
        api_key_candidates = [
            f"{provider.upper()}_API_KEY",  # ä¾‹å¦‚: KYX_API_KEY
            f"{provider}_API_KEY",          # ä¾‹å¦‚: kyx_API_KEY
            "CUSTOM_OPENAI_API_KEY"         # é€šç”¨çŽ¯å¢ƒå˜é‡
        ]

        custom_api_key = None
        for env_var in api_key_candidates:
            custom_api_key = os.getenv(env_var)
            if custom_api_key:
                logger.info(f"âœ… ä»ŽçŽ¯å¢ƒå˜é‡ {env_var} èŽ·å–åˆ° API Key")
                break

        if not custom_api_key:
            logger.warning(f"âš ï¸ æœªæ‰¾åˆ°è‡ªå®šä¹‰åŽ‚å®¶ {provider} çš„ API Keyï¼Œå°è¯•ä½¿ç”¨é»˜è®¤é…ç½®")

        return ChatOpenAI(
            model=model,
            base_url=backend_url,
            api_key=custom_api_key,
            temperature=temperature,
            max_tokens=max_tokens,
            timeout=timeout
        )


class TradingAgentsGraph:
    """Main class that orchestrates the trading agents framework."""

    def __init__(
        self,
        selected_analysts=None,
        debug=False,
        config: Dict[str, Any] = None,
    ):
        """Initialize the trading agents graph and components.

        Args:
            selected_analysts: List of analyst types to include
            debug: Whether to run in debug mode
            config: Configuration dictionary. If None, uses default config
        """
        self.debug = debug
        self.config = config or DEFAULT_CONFIG
        if not selected_analysts:
            raise ValueError("selected_analysts ä¸èƒ½ä¸ºç©ºï¼Œè¯·å…ˆé…ç½®é˜¶æ®µ1åˆ†æžå¸ˆã€‚")

        # å¦‚æžœå¤–éƒ¨å·²æ³¨å…¥ loader ä½†æœªæ˜¾å¼å¼€å¯å¼€å…³ï¼Œåˆ™è‡ªåŠ¨å¼€å¯
        if self.config.get("mcp_tool_loader") and not self.config.get("enable_mcp", False):
            self.config["enable_mcp"] = True
            logger.info("ðŸ”§ [TradingGraph] æ£€æµ‹åˆ° MCP loaderï¼Œå·²è‡ªåŠ¨å¯ç”¨ MCP å·¥å…·")

        # Update the interface's config
        set_config(self.config)

        # Create necessary directories
        os.makedirs(
            os.path.join(self.config["project_dir"], "dataflows/data_cache"),
            exist_ok=True,
        )

        # Initialize LLMs
        # ðŸ”§ ä»Žé…ç½®ä¸­è¯»å–æ¨¡åž‹å‚æ•°ï¼ˆä¼˜å…ˆä½¿ç”¨ç”¨æˆ·é…ç½®ï¼Œå¦åˆ™ä½¿ç”¨é»˜è®¤å€¼ï¼‰
        quick_config = self.config.get("quick_model_config", {})
        deep_config = self.config.get("deep_model_config", {})

        # è¯»å–å¿«é€Ÿæ¨¡åž‹å‚æ•°
        quick_max_tokens = quick_config.get("max_tokens", 4000)
        quick_temperature = quick_config.get("temperature", 0.7)
        quick_timeout = quick_config.get("timeout", 180)

        # è¯»å–æ·±åº¦æ¨¡åž‹å‚æ•°
        deep_max_tokens = deep_config.get("max_tokens", 4000)
        deep_temperature = deep_config.get("temperature", 0.7)
        deep_timeout = deep_config.get("timeout", 180)

        # ðŸ”§ æ£€æŸ¥æ˜¯å¦ä¸ºæ··åˆæ¨¡å¼ï¼ˆå¿«é€Ÿæ¨¡åž‹å’Œæ·±åº¦æ¨¡åž‹æ¥è‡ªä¸åŒåŽ‚å®¶ï¼‰
        quick_provider = self.config.get("quick_provider")
        deep_provider = self.config.get("deep_provider")
        quick_backend_url = self.config.get("quick_backend_url")
        deep_backend_url = self.config.get("deep_backend_url")

        if quick_provider and deep_provider and quick_provider != deep_provider:
            # æ··åˆæ¨¡å¼ï¼šå¿«é€Ÿæ¨¡åž‹å’Œæ·±åº¦æ¨¡åž‹æ¥è‡ªä¸åŒåŽ‚å®¶
            logger.info(f"ðŸ”€ [æ··åˆæ¨¡å¼] æ£€æµ‹åˆ°ä¸åŒåŽ‚å®¶çš„æ¨¡åž‹ç»„åˆ")
            logger.info(f"   å¿«é€Ÿæ¨¡åž‹: {self.config['quick_think_llm']} ({quick_provider})")
            logger.info(f"   æ·±åº¦æ¨¡åž‹: {self.config['deep_think_llm']} ({deep_provider})")

            # ä½¿ç”¨ç»Ÿä¸€çš„å‡½æ•°åˆ›å»º LLM å®žä¾‹
            self.quick_thinking_llm = create_llm_by_provider(
                provider=quick_provider,
                model=self.config["quick_think_llm"],
                backend_url=quick_backend_url or self.config.get("backend_url", ""),
                temperature=quick_temperature,
                max_tokens=quick_max_tokens,
                timeout=quick_timeout,
                api_key=self.config.get("quick_api_key")  # ðŸ”¥ ä¼ é€’ API Key
            )

            self.deep_thinking_llm = create_llm_by_provider(
                provider=deep_provider,
                model=self.config["deep_think_llm"],
                backend_url=deep_backend_url or self.config.get("backend_url", ""),
                temperature=deep_temperature,
                max_tokens=deep_max_tokens,
                timeout=deep_timeout,
                api_key=self.config.get("deep_api_key")  # ðŸ”¥ ä¼ é€’ API Key
            )

            logger.info(f"âœ… [æ··åˆæ¨¡å¼] LLM å®žä¾‹åˆ›å»ºæˆåŠŸ")

        elif self.config["llm_provider"].lower() == "openai":
            logger.info(f"ðŸ”§ [OpenAI-å¿«é€Ÿæ¨¡åž‹] max_tokens={quick_max_tokens}, temperature={quick_temperature}, timeout={quick_timeout}s")
            logger.info(f"ðŸ”§ [OpenAI-æ·±åº¦æ¨¡åž‹] max_tokens={deep_max_tokens}, temperature={deep_temperature}, timeout={deep_timeout}s")

            self.deep_thinking_llm = ChatOpenAI(
                model=self.config["deep_think_llm"],
                base_url=self.config["backend_url"],
                temperature=deep_temperature,
                max_tokens=deep_max_tokens,
                timeout=deep_timeout
            )
            self.quick_thinking_llm = ChatOpenAI(
                model=self.config["quick_think_llm"],
                base_url=self.config["backend_url"],
                temperature=quick_temperature,
                max_tokens=quick_max_tokens,
                timeout=quick_timeout
            )
        elif self.config["llm_provider"] == "siliconflow":
            # SiliconFlowæ”¯æŒï¼šä½¿ç”¨OpenAIå…¼å®¹API
            siliconflow_api_key = os.getenv('SILICONFLOW_API_KEY')
            if not siliconflow_api_key:
                raise ValueError("ä½¿ç”¨SiliconFlowéœ€è¦è®¾ç½®SILICONFLOW_API_KEYçŽ¯å¢ƒå˜é‡")

            logger.info(f"ðŸŒ [SiliconFlow] ä½¿ç”¨APIå¯†é’¥: {siliconflow_api_key[:20]}...")
            logger.info(f"ðŸ”§ [SiliconFlow-å¿«é€Ÿæ¨¡åž‹] max_tokens={quick_max_tokens}, temperature={quick_temperature}, timeout={quick_timeout}s")
            logger.info(f"ðŸ”§ [SiliconFlow-æ·±åº¦æ¨¡åž‹] max_tokens={deep_max_tokens}, temperature={deep_temperature}, timeout={deep_timeout}s")

            self.deep_thinking_llm = ChatOpenAI(
                model=self.config["deep_think_llm"],
                base_url=self.config["backend_url"],
                api_key=siliconflow_api_key,
                temperature=deep_temperature,
                max_tokens=deep_max_tokens,
                timeout=deep_timeout
            )
            self.quick_thinking_llm = ChatOpenAI(
                model=self.config["quick_think_llm"],
                base_url=self.config["backend_url"],
                api_key=siliconflow_api_key,
                temperature=quick_temperature,
                max_tokens=quick_max_tokens,
                timeout=quick_timeout
            )
        elif self.config["llm_provider"] == "openrouter":
            # OpenRouteræ”¯æŒï¼šä¼˜å…ˆä½¿ç”¨OPENROUTER_API_KEYï¼Œå¦åˆ™ä½¿ç”¨OPENAI_API_KEY
            openrouter_api_key = os.getenv('OPENROUTER_API_KEY') or os.getenv('OPENAI_API_KEY')
            if not openrouter_api_key:
                raise ValueError("ä½¿ç”¨OpenRouteréœ€è¦è®¾ç½®OPENROUTER_API_KEYæˆ–OPENAI_API_KEYçŽ¯å¢ƒå˜é‡")

            logger.info(f"ðŸŒ [OpenRouter] ä½¿ç”¨APIå¯†é’¥: {openrouter_api_key[:20]}...")
            logger.info(f"ðŸ”§ [OpenRouter-å¿«é€Ÿæ¨¡åž‹] max_tokens={quick_max_tokens}, temperature={quick_temperature}, timeout={quick_timeout}s")
            logger.info(f"ðŸ”§ [OpenRouter-æ·±åº¦æ¨¡åž‹] max_tokens={deep_max_tokens}, temperature={deep_temperature}, timeout={deep_timeout}s")

            self.deep_thinking_llm = ChatOpenAI(
                model=self.config["deep_think_llm"],
                base_url=self.config["backend_url"],
                api_key=openrouter_api_key,
                temperature=deep_temperature,
                max_tokens=deep_max_tokens,
                timeout=deep_timeout
            )
            self.quick_thinking_llm = ChatOpenAI(
                model=self.config["quick_think_llm"],
                base_url=self.config["backend_url"],
                api_key=openrouter_api_key,
                temperature=quick_temperature,
                max_tokens=quick_max_tokens,
                timeout=quick_timeout
            )
        elif self.config["llm_provider"] == "ollama":
            logger.info(f"ðŸ”§ [Ollama-å¿«é€Ÿæ¨¡åž‹] max_tokens={quick_max_tokens}, temperature={quick_temperature}, timeout={quick_timeout}s")
            logger.info(f"ðŸ”§ [Ollama-æ·±åº¦æ¨¡åž‹] max_tokens={deep_max_tokens}, temperature={deep_temperature}, timeout={deep_timeout}s")

            self.deep_thinking_llm = ChatOpenAI(
                model=self.config["deep_think_llm"],
                base_url=self.config["backend_url"],
                temperature=deep_temperature,
                max_tokens=deep_max_tokens,
                timeout=deep_timeout
            )
            self.quick_thinking_llm = ChatOpenAI(
                model=self.config["quick_think_llm"],
                base_url=self.config["backend_url"],
                temperature=quick_temperature,
                max_tokens=quick_max_tokens,
                timeout=quick_timeout
            )
        elif self.config["llm_provider"].lower() == "anthropic":
            logger.info(f"ðŸ”§ [Anthropic-å¿«é€Ÿæ¨¡åž‹] max_tokens={quick_max_tokens}, temperature={quick_temperature}, timeout={quick_timeout}s")
            logger.info(f"ðŸ”§ [Anthropic-æ·±åº¦æ¨¡åž‹] max_tokens={deep_max_tokens}, temperature={deep_temperature}, timeout={deep_timeout}s")

            self.deep_thinking_llm = ChatAnthropic(
                model=self.config["deep_think_llm"],
                base_url=self.config["backend_url"],
                temperature=deep_temperature,
                max_tokens=deep_max_tokens,
                timeout=deep_timeout
            )
            self.quick_thinking_llm = ChatAnthropic(
                model=self.config["quick_think_llm"],
                base_url=self.config["backend_url"],
                temperature=quick_temperature,
                max_tokens=quick_max_tokens,
                timeout=quick_timeout
            )
        elif self.config["llm_provider"].lower() == "google":
            # ä½¿ç”¨ Google OpenAI å…¼å®¹é€‚é…å™¨ï¼Œè§£å†³å·¥å…·è°ƒç”¨æ ¼å¼ä¸åŒ¹é…é—®é¢˜
            logger.info(f"ðŸ”§ ä½¿ç”¨Google AI OpenAI å…¼å®¹é€‚é…å™¨ (è§£å†³å·¥å…·è°ƒç”¨é—®é¢˜)")

            # ðŸ”¥ ä¼˜å…ˆä½¿ç”¨æ•°æ®åº“é…ç½®çš„ API Keyï¼Œå¦åˆ™ä»ŽçŽ¯å¢ƒå˜é‡è¯»å–
            google_api_key = self.config.get("quick_api_key") or self.config.get("deep_api_key") or os.getenv('GOOGLE_API_KEY')
            if not google_api_key:
                raise ValueError("ä½¿ç”¨Google AIéœ€è¦åœ¨æ•°æ®åº“ä¸­é…ç½®API Keyæˆ–è®¾ç½®GOOGLE_API_KEYçŽ¯å¢ƒå˜é‡")

            logger.info(f"ðŸ”‘ [Google AI] API Key æ¥æº: {'æ•°æ®åº“é…ç½®' if self.config.get('quick_api_key') or self.config.get('deep_api_key') else 'çŽ¯å¢ƒå˜é‡'}")

            # ðŸ”§ ä»Žé…ç½®ä¸­è¯»å–æ¨¡åž‹å‚æ•°ï¼ˆä¼˜å…ˆä½¿ç”¨ç”¨æˆ·é…ç½®ï¼Œå¦åˆ™ä½¿ç”¨é»˜è®¤å€¼ï¼‰
            quick_config = self.config.get("quick_model_config", {})
            deep_config = self.config.get("deep_model_config", {})

            quick_max_tokens = quick_config.get("max_tokens", 4000)
            quick_temperature = quick_config.get("temperature", 0.7)
            quick_timeout = quick_config.get("timeout", 180)

            deep_max_tokens = deep_config.get("max_tokens", 4000)
            deep_temperature = deep_config.get("temperature", 0.7)
            deep_timeout = deep_config.get("timeout", 180)

            logger.info(f"ðŸ”§ [Google-å¿«é€Ÿæ¨¡åž‹] max_tokens={quick_max_tokens}, temperature={quick_temperature}, timeout={quick_timeout}s")
            logger.info(f"ðŸ”§ [Google-æ·±åº¦æ¨¡åž‹] max_tokens={deep_max_tokens}, temperature={deep_temperature}, timeout={deep_timeout}s")

            # èŽ·å– backend_urlï¼ˆå¦‚æžœé…ç½®ä¸­æœ‰çš„è¯ï¼‰
            backend_url = self.config.get("backend_url")
            if backend_url:
                logger.info(f"ðŸ”§ [Google AI] ä½¿ç”¨é…ç½®çš„ backend_url: {backend_url}")
            else:
                logger.info(f"ðŸ”§ [Google AI] æœªé…ç½® backend_urlï¼Œä½¿ç”¨é»˜è®¤ç«¯ç‚¹")

            self.deep_thinking_llm = ChatGoogleOpenAI(
                model=self.config["deep_think_llm"],
                google_api_key=google_api_key,
                base_url=backend_url if backend_url else None,
                temperature=deep_temperature,
                max_tokens=deep_max_tokens,
                timeout=deep_timeout
            )
            self.quick_thinking_llm = ChatGoogleOpenAI(
                model=self.config["quick_think_llm"],
                google_api_key=google_api_key,
                base_url=backend_url if backend_url else None,
                temperature=quick_temperature,
                max_tokens=quick_max_tokens,
                timeout=quick_timeout,
                transport="rest"
            )

            logger.info(f"âœ… [Google AI] å·²å¯ç”¨ä¼˜åŒ–çš„å·¥å…·è°ƒç”¨å’Œå†…å®¹æ ¼å¼å¤„ç†å¹¶åº”ç”¨ç”¨æˆ·é…ç½®çš„æ¨¡åž‹å‚æ•°")
        elif (self.config["llm_provider"].lower() == "dashscope" or
              self.config["llm_provider"].lower() == "alibaba" or
              "dashscope" in self.config["llm_provider"].lower() or
              "é˜¿é‡Œç™¾ç‚¼" in self.config["llm_provider"]):
            # ä½¿ç”¨ OpenAI å…¼å®¹é€‚é…å™¨ï¼Œæ”¯æŒåŽŸç”Ÿ Function Calling
            logger.info(f"ðŸ”§ ä½¿ç”¨é˜¿é‡Œç™¾ç‚¼ OpenAI å…¼å®¹é€‚é…å™¨ (æ”¯æŒåŽŸç”Ÿå·¥å…·è°ƒç”¨)")

            # ðŸ”¥ ä¼˜å…ˆä½¿ç”¨æ•°æ®åº“é…ç½®çš„ API Keyï¼Œå¦åˆ™ä»ŽçŽ¯å¢ƒå˜é‡è¯»å–
            dashscope_api_key = self.config.get("quick_api_key") or self.config.get("deep_api_key") or os.getenv('DASHSCOPE_API_KEY')
            logger.info(f"ðŸ”‘ [é˜¿é‡Œç™¾ç‚¼] API Key æ¥æº: {'æ•°æ®åº“é…ç½®' if self.config.get('quick_api_key') or self.config.get('deep_api_key') else 'çŽ¯å¢ƒå˜é‡'}")

            # ðŸ”§ ä»Žé…ç½®ä¸­è¯»å–æ¨¡åž‹å‚æ•°ï¼ˆä¼˜å…ˆä½¿ç”¨ç”¨æˆ·é…ç½®ï¼Œå¦åˆ™ä½¿ç”¨é»˜è®¤å€¼ï¼‰
            quick_config = self.config.get("quick_model_config", {})
            deep_config = self.config.get("deep_model_config", {})

            # è¯»å–å¿«é€Ÿæ¨¡åž‹å‚æ•°
            quick_max_tokens = quick_config.get("max_tokens", 4000)
            quick_temperature = quick_config.get("temperature", 0.7)
            quick_timeout = quick_config.get("timeout", 180)

            # è¯»å–æ·±åº¦æ¨¡åž‹å‚æ•°
            deep_max_tokens = deep_config.get("max_tokens", 4000)
            deep_temperature = deep_config.get("temperature", 0.7)
            deep_timeout = deep_config.get("timeout", 180)

            logger.info(f"ðŸ”§ [é˜¿é‡Œç™¾ç‚¼-å¿«é€Ÿæ¨¡åž‹] max_tokens={quick_max_tokens}, temperature={quick_temperature}, timeout={quick_timeout}s")
            logger.info(f"ðŸ”§ [é˜¿é‡Œç™¾ç‚¼-æ·±åº¦æ¨¡åž‹] max_tokens={deep_max_tokens}, temperature={deep_temperature}, timeout={deep_timeout}s")

            # èŽ·å– backend_urlï¼ˆå¦‚æžœé…ç½®ä¸­æœ‰çš„è¯ï¼‰
            backend_url = self.config.get("backend_url")
            if backend_url:
                logger.info(f"ðŸ”§ [é˜¿é‡Œç™¾ç‚¼] ä½¿ç”¨è‡ªå®šä¹‰ API åœ°å€: {backend_url}")

            # ðŸ”¥ è¯¦ç»†æ—¥å¿—ï¼šæ‰“å°æ‰€æœ‰ LLM åˆå§‹åŒ–å‚æ•°
            logger.info("=" * 80)
            logger.info("ðŸ¤– [LLMåˆå§‹åŒ–] é˜¿é‡Œç™¾ç‚¼æ·±åº¦æ¨¡åž‹å‚æ•°:")
            logger.info(f"   model: {self.config['deep_think_llm']}")
            logger.info(f"   api_key: {'æœ‰å€¼' if dashscope_api_key else 'ç©º'} (é•¿åº¦: {len(dashscope_api_key) if dashscope_api_key else 0})")
            logger.info(f"   base_url: {backend_url if backend_url else 'é»˜è®¤'}")
            logger.info(f"   temperature: {deep_temperature}")
            logger.info(f"   max_tokens: {deep_max_tokens}")
            logger.info(f"   request_timeout: {deep_timeout}")
            logger.info("=" * 80)

            self.deep_thinking_llm = ChatDashScopeOpenAI(
                model=self.config["deep_think_llm"],
                api_key=dashscope_api_key,  # ðŸ”¥ ä¼ é€’ API Key
                base_url=backend_url if backend_url else None,  # ä¼ é€’ base_url
                temperature=deep_temperature,
                max_tokens=deep_max_tokens,
                request_timeout=deep_timeout
            )

            logger.info("=" * 80)
            logger.info("ðŸ¤– [LLMåˆå§‹åŒ–] é˜¿é‡Œç™¾ç‚¼å¿«é€Ÿæ¨¡åž‹å‚æ•°:")
            logger.info(f"   model: {self.config['quick_think_llm']}")
            logger.info(f"   api_key: {'æœ‰å€¼' if dashscope_api_key else 'ç©º'} (é•¿åº¦: {len(dashscope_api_key) if dashscope_api_key else 0})")
            logger.info(f"   base_url: {backend_url if backend_url else 'é»˜è®¤'}")
            logger.info(f"   temperature: {quick_temperature}")
            logger.info(f"   max_tokens: {quick_max_tokens}")
            logger.info(f"   request_timeout: {quick_timeout}")
            logger.info("=" * 80)

            self.quick_thinking_llm = ChatDashScopeOpenAI(
                model=self.config["quick_think_llm"],
                api_key=dashscope_api_key,  # ðŸ”¥ ä¼ é€’ API Key
                base_url=backend_url if backend_url else None,  # ä¼ é€’ base_url
                temperature=quick_temperature,
                max_tokens=quick_max_tokens,
                request_timeout=quick_timeout
            )
            logger.info(f"âœ… [é˜¿é‡Œç™¾ç‚¼] å·²åº”ç”¨ç”¨æˆ·é…ç½®çš„æ¨¡åž‹å‚æ•°")
        elif (self.config["llm_provider"].lower() == "deepseek" or
              "deepseek" in self.config["llm_provider"].lower()):
            # DeepSeek V3é…ç½® - ä½¿ç”¨æ”¯æŒtokenç»Ÿè®¡çš„é€‚é…å™¨
            from tradingagents.llm_adapters.deepseek_adapter import ChatDeepSeek

            deepseek_api_key = os.getenv('DEEPSEEK_API_KEY')
            if not deepseek_api_key:
                raise ValueError("ä½¿ç”¨DeepSeekéœ€è¦è®¾ç½®DEEPSEEK_API_KEYçŽ¯å¢ƒå˜é‡")

            deepseek_base_url = os.getenv('DEEPSEEK_BASE_URL', 'https://api.deepseek.com')

            # ðŸ”§ ä»Žé…ç½®ä¸­è¯»å–æ¨¡åž‹å‚æ•°ï¼ˆä¼˜å…ˆä½¿ç”¨ç”¨æˆ·é…ç½®ï¼Œå¦åˆ™ä½¿ç”¨é»˜è®¤å€¼ï¼‰
            quick_config = self.config.get("quick_model_config", {})
            deep_config = self.config.get("deep_model_config", {})

            # è¯»å–å¿«é€Ÿæ¨¡åž‹å‚æ•°
            quick_max_tokens = quick_config.get("max_tokens", 4000)
            quick_temperature = quick_config.get("temperature", 0.7)
            quick_timeout = quick_config.get("timeout", 180)

            # è¯»å–æ·±åº¦æ¨¡åž‹å‚æ•°
            deep_max_tokens = deep_config.get("max_tokens", 4000)
            deep_temperature = deep_config.get("temperature", 0.7)
            deep_timeout = deep_config.get("timeout", 180)

            logger.info(f"ðŸ”§ [DeepSeek-å¿«é€Ÿæ¨¡åž‹] max_tokens={quick_max_tokens}, temperature={quick_temperature}, timeout={quick_timeout}s")
            logger.info(f"ðŸ”§ [DeepSeek-æ·±åº¦æ¨¡åž‹] max_tokens={deep_max_tokens}, temperature={deep_temperature}, timeout={deep_timeout}s")

            # ä½¿ç”¨æ”¯æŒtokenç»Ÿè®¡çš„DeepSeeké€‚é…å™¨
            self.deep_thinking_llm = ChatDeepSeek(
                model=self.config["deep_think_llm"],
                api_key=deepseek_api_key,
                base_url=deepseek_base_url,
                temperature=deep_temperature,
                max_tokens=deep_max_tokens,
                timeout=deep_timeout
            )
            self.quick_thinking_llm = ChatDeepSeek(
                model=self.config["quick_think_llm"],
                api_key=deepseek_api_key,
                base_url=deepseek_base_url,
                temperature=quick_temperature,
                max_tokens=quick_max_tokens,
                timeout=quick_timeout
            )

            logger.info(f"âœ… [DeepSeek] å·²å¯ç”¨tokenç»Ÿè®¡åŠŸèƒ½å¹¶åº”ç”¨ç”¨æˆ·é…ç½®çš„æ¨¡åž‹å‚æ•°")
        elif self.config["llm_provider"].lower() == "custom_openai":
            # è‡ªå®šä¹‰OpenAIç«¯ç‚¹é…ç½®
            from tradingagents.llm_adapters.openai_compatible_base import create_openai_compatible_llm

            custom_api_key = os.getenv('CUSTOM_OPENAI_API_KEY')
            if not custom_api_key:
                raise ValueError("ä½¿ç”¨è‡ªå®šä¹‰OpenAIç«¯ç‚¹éœ€è¦è®¾ç½®CUSTOM_OPENAI_API_KEYçŽ¯å¢ƒå˜é‡")

            custom_base_url = self.config.get("custom_openai_base_url", "https://api.openai.com/v1")

            # ðŸ”§ ä»Žé…ç½®ä¸­è¯»å–æ¨¡åž‹å‚æ•°ï¼ˆä¼˜å…ˆä½¿ç”¨ç”¨æˆ·é…ç½®ï¼Œå¦åˆ™ä½¿ç”¨é»˜è®¤å€¼ï¼‰
            quick_config = self.config.get("quick_model_config", {})
            deep_config = self.config.get("deep_model_config", {})

            quick_max_tokens = quick_config.get("max_tokens", 4000)
            quick_temperature = quick_config.get("temperature", 0.7)
            quick_timeout = quick_config.get("timeout", 180)

            deep_max_tokens = deep_config.get("max_tokens", 4000)
            deep_temperature = deep_config.get("temperature", 0.7)
            deep_timeout = deep_config.get("timeout", 180)

            logger.info(f"ðŸ”§ [è‡ªå®šä¹‰OpenAI] ä½¿ç”¨ç«¯ç‚¹: {custom_base_url}")
            logger.info(f"ðŸ”§ [è‡ªå®šä¹‰OpenAI-å¿«é€Ÿæ¨¡åž‹] max_tokens={quick_max_tokens}, temperature={quick_temperature}, timeout={quick_timeout}s")
            logger.info(f"ðŸ”§ [è‡ªå®šä¹‰OpenAI-æ·±åº¦æ¨¡åž‹] max_tokens={deep_max_tokens}, temperature={deep_temperature}, timeout={deep_timeout}s")

            # ä½¿ç”¨OpenAIå…¼å®¹é€‚é…å™¨åˆ›å»ºLLMå®žä¾‹
            self.deep_thinking_llm = create_openai_compatible_llm(
                provider="custom_openai",
                model=self.config["deep_think_llm"],
                base_url=custom_base_url,
                temperature=deep_temperature,
                max_tokens=deep_max_tokens,
                timeout=deep_timeout
            )
            self.quick_thinking_llm = create_openai_compatible_llm(
                provider="custom_openai",
                model=self.config["quick_think_llm"],
                base_url=custom_base_url,
                temperature=quick_temperature,
                max_tokens=quick_max_tokens,
                timeout=quick_timeout
            )

            logger.info(f"âœ… [è‡ªå®šä¹‰OpenAI] å·²é…ç½®è‡ªå®šä¹‰ç«¯ç‚¹å¹¶åº”ç”¨ç”¨æˆ·é…ç½®çš„æ¨¡åž‹å‚æ•°")
        elif self.config["llm_provider"].lower() == "qianfan":
            # ç™¾åº¦åƒå¸†ï¼ˆæ–‡å¿ƒä¸€è¨€ï¼‰é…ç½® - ç»Ÿä¸€ç”±é€‚é…å™¨å†…éƒ¨è¯»å–ä¸Žæ ¡éªŒ QIANFAN_API_KEY
            from tradingagents.llm_adapters.openai_compatible_base import create_openai_compatible_llm

            # ðŸ”§ ä»Žé…ç½®ä¸­è¯»å–æ¨¡åž‹å‚æ•°ï¼ˆä¼˜å…ˆä½¿ç”¨ç”¨æˆ·é…ç½®ï¼Œå¦åˆ™ä½¿ç”¨é»˜è®¤å€¼ï¼‰
            quick_config = self.config.get("quick_model_config", {})
            deep_config = self.config.get("deep_model_config", {})

            quick_max_tokens = quick_config.get("max_tokens", 4000)
            quick_temperature = quick_config.get("temperature", 0.7)
            quick_timeout = quick_config.get("timeout", 180)

            deep_max_tokens = deep_config.get("max_tokens", 4000)
            deep_temperature = deep_config.get("temperature", 0.7)
            deep_timeout = deep_config.get("timeout", 180)

            logger.info(f"ðŸ”§ [åƒå¸†-å¿«é€Ÿæ¨¡åž‹] max_tokens={quick_max_tokens}, temperature={quick_temperature}, timeout={quick_timeout}s")
            logger.info(f"ðŸ”§ [åƒå¸†-æ·±åº¦æ¨¡åž‹] max_tokens={deep_max_tokens}, temperature={deep_temperature}, timeout={deep_timeout}s")

            # ä½¿ç”¨OpenAIå…¼å®¹é€‚é…å™¨åˆ›å»ºLLMå®žä¾‹ï¼ˆåŸºç±»ä¼šä½¿ç”¨åƒå¸†é»˜è®¤base_urlå¹¶è´Ÿè´£å¯†é’¥æ ¡éªŒï¼‰
            self.deep_thinking_llm = create_openai_compatible_llm(
                provider="qianfan",
                model=self.config["deep_think_llm"],
                temperature=deep_temperature,
                max_tokens=deep_max_tokens,
                timeout=deep_timeout
            )
            self.quick_thinking_llm = create_openai_compatible_llm(
                provider="qianfan",
                model=self.config["quick_think_llm"],
                temperature=quick_temperature,
                max_tokens=quick_max_tokens,
                timeout=quick_timeout
            )
            logger.info("âœ… [åƒå¸†] æ–‡å¿ƒä¸€è¨€é€‚é…å™¨å·²é…ç½®æˆåŠŸå¹¶åº”ç”¨ç”¨æˆ·é…ç½®çš„æ¨¡åž‹å‚æ•°")
        elif self.config["llm_provider"].lower() == "zhipu":
            # æ™ºè°±AI GLMé…ç½® - ä½¿ç”¨ä¸“é—¨çš„ChatZhipuOpenAIé€‚é…å™¨
            from tradingagents.llm_adapters.openai_compatible_base import ChatZhipuOpenAI
            
            # ðŸ”¥ ä¼˜å…ˆä½¿ç”¨æ•°æ®åº“é…ç½®çš„ API Keyï¼Œå¦åˆ™ä»ŽçŽ¯å¢ƒå˜é‡è¯»å–
            zhipu_api_key = self.config.get("quick_api_key") or self.config.get("deep_api_key") or os.getenv('ZHIPU_API_KEY')
            logger.info(f"ðŸ”‘ [æ™ºè°±AI] API Key æ¥æº: {'æ•°æ®åº“é…ç½®' if self.config.get('quick_api_key') or self.config.get('deep_api_key') else 'çŽ¯å¢ƒå˜é‡'}")
            
            if not zhipu_api_key:
                raise ValueError("ä½¿ç”¨æ™ºè°±AIéœ€è¦åœ¨æ•°æ®åº“ä¸­é…ç½®API Keyæˆ–è®¾ç½®ZHIPU_API_KEYçŽ¯å¢ƒå˜é‡")
            
            # ðŸ”§ ä»Žé…ç½®ä¸­è¯»å–æ¨¡åž‹å‚æ•°ï¼ˆä¼˜å…ˆä½¿ç”¨ç”¨æˆ·é…ç½®ï¼Œå¦åˆ™ä½¿ç”¨é»˜è®¤å€¼ï¼‰
            quick_config = self.config.get("quick_model_config", {})
            deep_config = self.config.get("deep_model_config", {})
            
            quick_max_tokens = quick_config.get("max_tokens", 4000)
            quick_temperature = quick_config.get("temperature", 0.7)
            quick_timeout = quick_config.get("timeout", 180)
            
            deep_max_tokens = deep_config.get("max_tokens", 4000)
            deep_temperature = deep_config.get("temperature", 0.7)
            deep_timeout = deep_config.get("timeout", 180)
            
            logger.info(f"ðŸ”§ [æ™ºè°±AI-å¿«é€Ÿæ¨¡åž‹] max_tokens={quick_max_tokens}, temperature={quick_temperature}, timeout={quick_timeout}s")
            logger.info(f"ðŸ”§ [æ™ºè°±AI-æ·±åº¦æ¨¡åž‹] max_tokens={deep_max_tokens}, temperature={deep_temperature}, timeout={deep_timeout}s")
            
            # èŽ·å– backend_urlï¼ˆå¦‚æžœé…ç½®ä¸­æœ‰çš„è¯ï¼‰
            backend_url = self.config.get("backend_url")
            if backend_url:
                logger.info(f"ðŸ”§ [æ™ºè°±AI] ä½¿ç”¨é…ç½®çš„ backend_url: {backend_url}")
            else:
                logger.info(f"ðŸ”§ [æ™ºè°±AI] æœªé…ç½® backend_urlï¼Œä½¿ç”¨é»˜è®¤ç«¯ç‚¹")
            
            # ä½¿ç”¨ä¸“é—¨çš„ChatZhipuOpenAIé€‚é…å™¨åˆ›å»ºLLMå®žä¾‹
            self.deep_thinking_llm = ChatZhipuOpenAI(
                model=self.config["deep_think_llm"],
                api_key=zhipu_api_key,
                base_url=backend_url,  # ä½¿ç”¨ç”¨æˆ·é…ç½®çš„backend_url
                temperature=deep_temperature,
                max_tokens=deep_max_tokens,
                timeout=deep_timeout
            )
            self.quick_thinking_llm = ChatZhipuOpenAI(
                model=self.config["quick_think_llm"],
                api_key=zhipu_api_key,
                base_url=backend_url,  # ä½¿ç”¨ç”¨æˆ·é…ç½®çš„backend_url
                temperature=quick_temperature,
                max_tokens=quick_max_tokens,
                timeout=quick_timeout
            )
            
            logger.info("âœ… [æ™ºè°±AI] å·²ä½¿ç”¨ä¸“ç”¨é€‚é…å™¨é…ç½®æˆåŠŸå¹¶åº”ç”¨ç”¨æˆ·é…ç½®çš„æ¨¡åž‹å‚æ•°")
        else:
            # ðŸ”§ é€šç”¨çš„ OpenAI å…¼å®¹åŽ‚å®¶æ”¯æŒï¼ˆç”¨äºŽè‡ªå®šä¹‰åŽ‚å®¶ï¼‰
            logger.info(f"ðŸ”§ ä½¿ç”¨é€šç”¨ OpenAI å…¼å®¹é€‚é…å™¨å¤„ç†è‡ªå®šä¹‰åŽ‚å®¶: {self.config['llm_provider']}")
            from tradingagents.llm_adapters.openai_compatible_base import create_openai_compatible_llm

            # èŽ·å–åŽ‚å®¶é…ç½®ä¸­çš„ API Key å’Œ base_url
            provider_name = self.config['llm_provider']

            # å°è¯•ä»ŽçŽ¯å¢ƒå˜é‡èŽ·å– API Keyï¼ˆæ”¯æŒå¤šç§å‘½åæ ¼å¼ï¼‰
            api_key_candidates = [
                f"{provider_name.upper()}_API_KEY",  # ä¾‹å¦‚: KYX_API_KEY
                f"{provider_name}_API_KEY",          # ä¾‹å¦‚: kyx_API_KEY
                "CUSTOM_OPENAI_API_KEY"              # é€šç”¨çŽ¯å¢ƒå˜é‡
            ]

            custom_api_key = None
            for env_var in api_key_candidates:
                custom_api_key = os.getenv(env_var)
                if custom_api_key:
                    logger.info(f"âœ… ä»ŽçŽ¯å¢ƒå˜é‡ {env_var} èŽ·å–åˆ° API Key")
                    break

            if not custom_api_key:
                raise ValueError(
                    f"ä½¿ç”¨è‡ªå®šä¹‰åŽ‚å®¶ {provider_name} éœ€è¦è®¾ç½®ä»¥ä¸‹çŽ¯å¢ƒå˜é‡ä¹‹ä¸€:\n"
                    f"  - {provider_name.upper()}_API_KEY\n"
                    f"  - CUSTOM_OPENAI_API_KEY"
                )

            # èŽ·å– backend_urlï¼ˆä»Žé…ç½®ä¸­èŽ·å–ï¼‰
            backend_url = self.config.get("backend_url")
            if not backend_url:
                raise ValueError(
                    f"ä½¿ç”¨è‡ªå®šä¹‰åŽ‚å®¶ {provider_name} éœ€è¦åœ¨æ•°æ®åº“é…ç½®ä¸­è®¾ç½® default_base_url"
                )

            logger.info(f"ðŸ”§ [è‡ªå®šä¹‰åŽ‚å®¶ {provider_name}] ä½¿ç”¨ç«¯ç‚¹: {backend_url}")

            # ðŸ”§ ä»Žé…ç½®ä¸­è¯»å–æ¨¡åž‹å‚æ•°
            quick_config = self.config.get("quick_model_config", {})
            deep_config = self.config.get("deep_model_config", {})

            quick_max_tokens = quick_config.get("max_tokens", 4000)
            quick_temperature = quick_config.get("temperature", 0.7)
            quick_timeout = quick_config.get("timeout", 180)

            deep_max_tokens = deep_config.get("max_tokens", 4000)
            deep_temperature = deep_config.get("temperature", 0.7)
            deep_timeout = deep_config.get("timeout", 180)

            logger.info(f"ðŸ”§ [{provider_name}-å¿«é€Ÿæ¨¡åž‹] max_tokens={quick_max_tokens}, temperature={quick_temperature}, timeout={quick_timeout}s")
            logger.info(f"ðŸ”§ [{provider_name}-æ·±åº¦æ¨¡åž‹] max_tokens={deep_max_tokens}, temperature={deep_temperature}, timeout={deep_timeout}s")

            # ä½¿ç”¨ custom_openai é€‚é…å™¨åˆ›å»º LLM å®žä¾‹
            self.deep_thinking_llm = create_openai_compatible_llm(
                provider="custom_openai",
                model=self.config["deep_think_llm"],
                api_key=custom_api_key,
                base_url=backend_url,
                temperature=deep_temperature,
                max_tokens=deep_max_tokens,
                timeout=deep_timeout
            )
            self.quick_thinking_llm = create_openai_compatible_llm(
                provider="custom_openai",
                model=self.config["quick_think_llm"],
                api_key=custom_api_key,
                base_url=backend_url,
                temperature=quick_temperature,
                max_tokens=quick_max_tokens,
                timeout=quick_timeout
            )

            logger.info(f"âœ… [è‡ªå®šä¹‰åŽ‚å®¶ {provider_name}] å·²é…ç½®è‡ªå®šä¹‰ç«¯ç‚¹å¹¶åº”ç”¨ç”¨æˆ·é…ç½®çš„æ¨¡åž‹å‚æ•°")
        
        self.toolkit = Toolkit(config=self.config)

        # Initialize memories (å¦‚æžœå¯ç”¨)
        memory_enabled = self.config.get("memory_enabled", True)
        if memory_enabled:
            # ä½¿ç”¨å•ä¾‹ChromaDBç®¡ç†å™¨ï¼Œé¿å…å¹¶å‘åˆ›å»ºå†²çª
            self.bull_memory = FinancialSituationMemory("bull_memory", self.config)
            self.bear_memory = FinancialSituationMemory("bear_memory", self.config)
            self.trader_memory = FinancialSituationMemory("trader_memory", self.config)
            self.invest_judge_memory = FinancialSituationMemory("invest_judge_memory", self.config)
            self.risk_manager_memory = FinancialSituationMemory("risk_manager_memory", self.config)
        else:
            # åˆ›å»ºç©ºçš„å†…å­˜å¯¹è±¡
            self.bull_memory = None
            self.bear_memory = None
            self.trader_memory = None
            self.invest_judge_memory = None
            self.risk_manager_memory = None

        # Create tool nodes
        self.tool_nodes = self._create_tool_nodes()

        # Initialize components
        # ðŸ”¥ [ä¿®å¤] ä»Žé…ç½®ä¸­è¯»å–è¾©è®ºè½®æ¬¡å‚æ•° (ä¼˜å…ˆä½¿ç”¨é˜¶æ®µé…ç½®)
        # æ³¨æ„ï¼šç”¨æˆ·é…ç½®çš„æ˜¯"è¾©è®ºè½®æ¬¡"ï¼ˆä¸å«åˆå§‹æŠ¥å‘Šï¼‰ï¼Œå†…éƒ¨é€»è¾‘éœ€è¦+1ï¼ˆåŒ…å«åˆå§‹æŠ¥å‘Šè½®ï¼‰
        max_debate_rounds = self.config.get("phase2_debate_rounds")
        if max_debate_rounds is None:
             max_debate_rounds = self.config.get("max_debate_rounds", 1)
        
        # ç¡®ä¿è½¬æ¢ä¸ºæ•´æ•°
        if max_debate_rounds is not None:
            max_debate_rounds = int(max_debate_rounds)
        
        if self.config.get("phase2_enabled") is False:
             max_debate_rounds = 0
             
        max_risk_rounds = self.config.get("phase3_debate_rounds")
        if max_risk_rounds is None:
             max_risk_rounds = self.config.get("max_risk_discuss_rounds", 1)
             
        if self.config.get("phase3_enabled") is False:
             max_risk_rounds = 0

        self.conditional_logic = ConditionalLogic(
            max_debate_rounds=max_debate_rounds,
            max_risk_discuss_rounds=max_risk_rounds
        )
        logger.info(f"ðŸ”§ [ConditionalLogic] åˆå§‹åŒ–å®Œæˆ:")
        logger.info(f"   - max_debate_rounds: {self.conditional_logic.max_debate_rounds}")
        logger.info(f"   - max_risk_discuss_rounds: {self.conditional_logic.max_risk_discuss_rounds}")

        self.graph_setup = GraphSetup(
            self.quick_thinking_llm,
            self.deep_thinking_llm,
            self.toolkit,
            self.tool_nodes,
            self.bull_memory,
            self.bear_memory,
            self.trader_memory,
            self.invest_judge_memory,
            self.risk_manager_memory,
            self.conditional_logic,
            self.config,
            getattr(self, 'react_llm', None),
        )

        self.propagator = Propagator()
        self.reflector = Reflector(self.quick_thinking_llm)
        self.signal_processor = SignalProcessor(self.quick_thinking_llm)

        # State tracking
        self.curr_state = None
        self.ticker = None
        self.log_states_dict = {}  # date to full state dict

        # Set up the graph
        self.graph = self.graph_setup.setup_graph(selected_analysts)

    def _create_tool_nodes(self) -> Dict[str, ToolNode]:
        """Create tool nodes for different data sources.

        æ³¨æ„ï¼šToolNode åŒ…å«æ‰€æœ‰å¯èƒ½çš„å·¥å…·ï¼Œä½† LLM åªä¼šè°ƒç”¨å®ƒç»‘å®šçš„å·¥å…·ã€‚
        ToolNode çš„ä½œç”¨æ˜¯æ‰§è¡Œ LLM ç”Ÿæˆçš„ tool_callsï¼Œè€Œä¸æ˜¯é™åˆ¶ LLM å¯ä»¥è°ƒç”¨å“ªäº›å·¥å…·ã€‚
        """
        # èŽ·å– MCP å·¥å…·ï¼ˆæŒ‰éœ€åŠ è½½ï¼‰
        mcp_tools = []
        if self.config.get("enable_mcp", False):
            loader = self.config.get("mcp_tool_loader")
            if callable(loader):
                try:
                    mcp_tools = list(loader())
                    # RunnableBinding tools are standard in LangChain MCP adapters
                    # No fixing needed for LangGraph if using updated versions
                    logger.info(f"ðŸ”§ [TradingGraph] å‘æ‰€æœ‰å·¥å…·èŠ‚ç‚¹æ³¨å…¥ {len(mcp_tools)} ä¸ª MCP å·¥å…·")
                except Exception as exc:  # pragma: no cover - è¿è¡Œæ—¶ä¿æŠ¤
                    logger.error(f"[TradingGraph] MCP å·¥å…·åŠ è½½å¤±è´¥: {exc}")
            else:
                logger.warning("[TradingGraph] å·²å¼€å¯ MCPï¼Œä½†æœªæä¾›æœ‰æ•ˆ loaderï¼Œè·³è¿‡åŠ è½½")

        return {
            "market": ToolNode(
                [
                    # ç»Ÿä¸€å·¥å…·ï¼ˆæŽ¨èï¼‰
                    self.toolkit.get_stock_market_data_unified,
                    # åœ¨çº¿å·¥å…·ï¼ˆå¤‡ç”¨ï¼‰
                    self.toolkit.get_YFin_data_online,
                    self.toolkit.get_stockstats_indicators_report_online,
                    # ç¦»çº¿å·¥å…·ï¼ˆå¤‡ç”¨ï¼‰
                    self.toolkit.get_YFin_data,
                    self.toolkit.get_stockstats_indicators_report,
                ] + mcp_tools
            ),
            "social": ToolNode(
                [
                    # ç»Ÿä¸€å·¥å…·ï¼ˆæŽ¨èï¼‰
                    self.toolkit.get_stock_sentiment_unified,
                    # åœ¨çº¿å·¥å…·ï¼ˆå¤‡ç”¨ï¼‰
                    self.toolkit.get_stock_news_openai,
                    # ç¦»çº¿å·¥å…·ï¼ˆå¤‡ç”¨ï¼‰
                    self.toolkit.get_reddit_stock_info,
                ] + mcp_tools
            ),
            "news": ToolNode(
                [
                    # ç»Ÿä¸€å·¥å…·ï¼ˆæŽ¨èï¼‰
                    self.toolkit.get_stock_news_unified,
                    # åœ¨çº¿å·¥å…·ï¼ˆå¤‡ç”¨ï¼‰
                    self.toolkit.get_global_news_openai,
                    self.toolkit.get_google_news,
                    # ç¦»çº¿å·¥å…·ï¼ˆå¤‡ç”¨ï¼‰
                    self.toolkit.get_finnhub_news,
                    self.toolkit.get_reddit_news,
                ] + mcp_tools
            ),
            "fundamentals": ToolNode(
                [
                    # ç»Ÿä¸€å·¥å…·ï¼ˆæŽ¨èï¼‰
                    self.toolkit.get_stock_fundamentals_unified,
                    # ç¦»çº¿å·¥å…·ï¼ˆå¤‡ç”¨ï¼‰
                    self.toolkit.get_finnhub_company_insider_sentiment,
                    self.toolkit.get_finnhub_company_insider_transactions,
                    self.toolkit.get_simfin_balance_sheet,
                    self.toolkit.get_simfin_cashflow,
                    self.toolkit.get_simfin_income_stmt,
                    # ä¸­å›½å¸‚åœºå·¥å…·ï¼ˆå¤‡ç”¨ï¼‰
                    self.toolkit.get_china_stock_data,
                    self.toolkit.get_china_fundamentals,
                ] + mcp_tools
            ),
        }

    def propagate(self, company_name, trade_date, progress_callback=None, task_id=None):
        """Run the trading agents graph for a company on a specific date.

        Args:
            company_name: Company name or stock symbol
            trade_date: Date for analysis
            progress_callback: Optional callback function for progress updates
            task_id: Optional task ID for tracking performance data
        """

        # æ·»åŠ è¯¦ç»†çš„æŽ¥æ”¶æ—¥å¿—
        logger.debug(f"ðŸ” [GRAPH DEBUG] ===== TradingAgentsGraph.propagate æŽ¥æ”¶å‚æ•° =====")
        logger.debug(f"ðŸ” [GRAPH DEBUG] æŽ¥æ”¶åˆ°çš„company_name: '{company_name}' (ç±»åž‹: {type(company_name)})")
        logger.debug(f"ðŸ” [GRAPH DEBUG] æŽ¥æ”¶åˆ°çš„trade_date: '{trade_date}' (ç±»åž‹: {type(trade_date)})")
        logger.debug(f"ðŸ” [GRAPH DEBUG] æŽ¥æ”¶åˆ°çš„task_id: '{task_id}'")

        self.ticker = company_name
        logger.debug(f"ðŸ” [GRAPH DEBUG] è®¾ç½®self.ticker: '{self.ticker}'")

        # Initialize state
        logger.debug(f"ðŸ” [GRAPH DEBUG] åˆ›å»ºåˆå§‹çŠ¶æ€ï¼Œä¼ é€’å‚æ•°: company_name='{company_name}', trade_date='{trade_date}'")
        init_agent_state = self.propagator.create_initial_state(
            company_name, trade_date
        )
        
        # æ³¨å…¥é˜¶æ®µé…ç½®å‚æ•°åˆ°åˆå§‹çŠ¶æ€ (ä»Ž config ä¸­è¯»å–å¹¶æ³¨å…¥)
        init_agent_state["phase2_enabled"] = self.config.get("phase2_enabled", False)
        init_agent_state["phase3_enabled"] = self.config.get("phase3_enabled", False)
        init_agent_state["phase4_enabled"] = self.config.get("phase4_enabled", False)
        
        logger.debug(f"ðŸ” [GRAPH DEBUG] åˆå§‹çŠ¶æ€ä¸­çš„company_of_interest: '{init_agent_state.get('company_of_interest', 'NOT_FOUND')}'")
        logger.debug(f"ðŸ” [GRAPH DEBUG] åˆå§‹çŠ¶æ€ä¸­çš„trade_date: '{init_agent_state.get('trade_date', 'NOT_FOUND')}'")
        logger.debug(f"ðŸ” [GRAPH DEBUG] é˜¶æ®µé…ç½®æ³¨å…¥çŠ¶æ€: P2={init_agent_state['phase2_enabled']}, P3={init_agent_state['phase3_enabled']}, P4={init_agent_state['phase4_enabled']}")

        # åˆå§‹åŒ–è®¡æ—¶å™¨
        node_timings = {}  # è®°å½•æ¯ä¸ªèŠ‚ç‚¹çš„æ‰§è¡Œæ—¶é—´
        total_start_time = time.time()  # æ€»ä½“å¼€å§‹æ—¶é—´
        current_node_start = None  # å½“å‰èŠ‚ç‚¹å¼€å§‹æ—¶é—´
        current_node_name = None  # å½“å‰èŠ‚ç‚¹åç§°

        # ä¿å­˜task_idç”¨äºŽåŽç»­ä¿å­˜æ€§èƒ½æ•°æ®
        self._current_task_id = task_id

        # æ ¹æ®æ˜¯å¦æœ‰è¿›åº¦å›žè°ƒé€‰æ‹©ä¸åŒçš„stream_mode
        args = self.propagator.get_graph_args(use_progress_callback=bool(progress_callback))

        def _merge_state_update(target: Dict[str, Any], update: Dict[str, Any]) -> None:
            """
            Safely mergeèŠ‚ç‚¹å¢žé‡åˆ°æœ€ç»ˆçŠ¶æ€ï¼Œç¡®ä¿ reports å­—å…¸ä¸ä¼šè¢«åŽç»­èŠ‚ç‚¹è¦†ç›–ã€‚

            åœ¨ stream_mode='updates' ä¸‹ï¼Œchunk åªåŒ…å«å¢žé‡å­—æ®µï¼Œç›´æŽ¥ dict.update
            ä¼šå¯¼è‡´ reports è¢«æœ€åŽä¸€ä¸ªèŠ‚ç‚¹è¦†ç›–ï¼ŒåŠ¨æ€æ™ºèƒ½ä½“çš„æŠ¥å‘Šä¸¢å¤±ã€‚
            """
            if target is None or not update:
                return

            # åˆå¹¶ reports å­—å…¸
            if "reports" in update and isinstance(update.get("reports"), dict):
                existing_reports = target.get("reports") or {}
                target["reports"] = {**existing_reports, **update["reports"]}

            # å¤„ç†å…¶å®ƒå­—æ®µï¼ˆä¿æŒå¢žé‡è¦†ç›–è¯­ä¹‰ï¼‰
            for k, v in update.items():
                if k == "reports":
                    continue
                target[k] = v

        if self.debug:
            # Debug mode with tracing and progress updates
            trace = []
            final_state = None
            for chunk in self.graph.stream(init_agent_state, **args):
                # è®°å½•èŠ‚ç‚¹è®¡æ—¶
                for node_name in chunk.keys():
                    if not node_name.startswith('__'):
                        # å¦‚æžœæœ‰ä¸Šä¸€ä¸ªèŠ‚ç‚¹ï¼Œè®°å½•å…¶ç»“æŸæ—¶é—´
                        if current_node_name and current_node_start:
                            elapsed = time.time() - current_node_start
                            node_timings[current_node_name] = elapsed
                            logger.info(f"â±ï¸ [{current_node_name}] è€—æ—¶: {elapsed:.2f}ç§’")

                        # å¼€å§‹æ–°èŠ‚ç‚¹è®¡æ—¶
                        current_node_name = node_name
                        current_node_start = time.time()
                        break

                # åœ¨ updates æ¨¡å¼ä¸‹ï¼Œchunk æ ¼å¼ä¸º {node_name: state_update}
                # åœ¨ values æ¨¡å¼ä¸‹ï¼Œchunk æ ¼å¼ä¸ºå®Œæ•´çš„çŠ¶æ€
                if progress_callback and args.get("stream_mode") == "updates":
                    # updates æ¨¡å¼ï¼šchunk = {"Market Analyst": {...}}
                    self._send_progress_update(chunk, progress_callback)
                    # ç´¯ç§¯çŠ¶æ€æ›´æ–°
                    if final_state is None:
                        final_state = init_agent_state.copy()
                    for node_name, node_update in chunk.items():
                        if not node_name.startswith('__'):
                            _merge_state_update(final_state, node_update)
                else:
                    # values æ¨¡å¼ï¼šchunk = {"messages": [...], ...}
                    if len(chunk.get("messages", [])) > 0:
                        chunk["messages"][-1].pretty_print()
                    trace.append(chunk)
                    final_state = chunk

            if not trace and final_state:
                # updates æ¨¡å¼ä¸‹ï¼Œä½¿ç”¨ç´¯ç§¯çš„çŠ¶æ€
                pass
            elif trace:
                final_state = trace[-1]
        else:
            # Standard mode without tracing but with progress updates
            if progress_callback:
                # ä½¿ç”¨ updates æ¨¡å¼ä»¥ä¾¿èŽ·å–èŠ‚ç‚¹çº§åˆ«çš„è¿›åº¦
                trace = []
                final_state = None
                for chunk in self.graph.stream(init_agent_state, **args):
                    # è®°å½•èŠ‚ç‚¹è®¡æ—¶
                    for node_name in chunk.keys():
                        if not node_name.startswith('__'):
                            # å¦‚æžœæœ‰ä¸Šä¸€ä¸ªèŠ‚ç‚¹ï¼Œè®°å½•å…¶ç»“æŸæ—¶é—´
                            if current_node_name and current_node_start:
                                elapsed = time.time() - current_node_start
                                node_timings[current_node_name] = elapsed
                                logger.info(f"â±ï¸ [{current_node_name}] è€—æ—¶: {elapsed:.2f}ç§’")
                                logger.info(f"ðŸ” [TIMING] èŠ‚ç‚¹åˆ‡æ¢: {current_node_name} â†’ {node_name}")

                            # å¼€å§‹æ–°èŠ‚ç‚¹è®¡æ—¶
                            current_node_name = node_name
                            current_node_start = time.time()
                            logger.info(f"ðŸ” [TIMING] å¼€å§‹è®¡æ—¶: {node_name}")
                            break

                    self._send_progress_update(chunk, progress_callback)
                    # ç´¯ç§¯çŠ¶æ€æ›´æ–°
                    if final_state is None:
                        final_state = init_agent_state.copy()
                    for node_name, node_update in chunk.items():
                        if not node_name.startswith('__'):
                            _merge_state_update(final_state, node_update)
            else:
                # åŽŸæœ‰çš„invokeæ¨¡å¼ï¼ˆä¹Ÿéœ€è¦è®¡æ—¶ï¼‰
                logger.info("â±ï¸ ä½¿ç”¨ invoke æ¨¡å¼æ‰§è¡Œåˆ†æžï¼ˆæ— è¿›åº¦å›žè°ƒï¼‰")
                # ä½¿ç”¨streamæ¨¡å¼ä»¥ä¾¿è®¡æ—¶ï¼Œä½†ä¸å‘é€è¿›åº¦æ›´æ–°
                trace = []
                final_state = None
                for chunk in self.graph.stream(init_agent_state, **args):
                    # è®°å½•èŠ‚ç‚¹è®¡æ—¶
                    for node_name in chunk.keys():
                        if not node_name.startswith('__'):
                            # å¦‚æžœæœ‰ä¸Šä¸€ä¸ªèŠ‚ç‚¹ï¼Œè®°å½•å…¶ç»“æŸæ—¶é—´
                            if current_node_name and current_node_start:
                                elapsed = time.time() - current_node_start
                                node_timings[current_node_name] = elapsed
                                logger.info(f"â±ï¸ [{current_node_name}] è€—æ—¶: {elapsed:.2f}ç§’")

                            # å¼€å§‹æ–°èŠ‚ç‚¹è®¡æ—¶
                            current_node_name = node_name
                            current_node_start = time.time()
                            break

                    # ç´¯ç§¯çŠ¶æ€æ›´æ–°
                    if final_state is None:
                        final_state = init_agent_state.copy()
                    for node_name, node_update in chunk.items():
                        if not node_name.startswith('__'):
                            final_state.update(node_update)

        # è®°å½•æœ€åŽä¸€ä¸ªèŠ‚ç‚¹çš„æ—¶é—´
        if current_node_name and current_node_start:
            elapsed = time.time() - current_node_start
            node_timings[current_node_name] = elapsed
            logger.info(f"â±ï¸ [{current_node_name}] è€—æ—¶: {elapsed:.2f}ç§’")

        # ðŸ”¥ å°† reports å­—å…¸ä¸­çš„åŠ¨æ€æŠ¥å‘Šå›žå¡«åˆ°é¡¶å±‚ *_report å­—æ®µï¼ˆæ”¯æŒè‡ªå®šä¹‰æ™ºèƒ½ä½“ï¼‰
        merged_reports = final_state.get("reports") or {}
        for report_key, report_content in merged_reports.items():
            if (
                report_key.endswith("_report")
                and report_content
                and not final_state.get(report_key)
            ):
                final_state[report_key] = report_content

        # è®¡ç®—æ€»æ—¶é—´
        total_elapsed = time.time() - total_start_time

        # è°ƒè¯•æ—¥å¿—
        logger.info(f"ðŸ” [TIMING DEBUG] èŠ‚ç‚¹è®¡æ—¶æ•°é‡: {len(node_timings)}")
        logger.info(f"ðŸ” [TIMING DEBUG] æ€»è€—æ—¶: {total_elapsed:.2f}ç§’")
        logger.info(f"ðŸ” [TIMING DEBUG] èŠ‚ç‚¹åˆ—è¡¨: {list(node_timings.keys())}")

        # æ‰“å°è¯¦ç»†çš„æ—¶é—´ç»Ÿè®¡
        logger.info("ðŸ” [TIMING DEBUG] å‡†å¤‡è°ƒç”¨ _print_timing_summary")
        self._print_timing_summary(node_timings, total_elapsed)
        logger.info("ðŸ” [TIMING DEBUG] _print_timing_summary è°ƒç”¨å®Œæˆ")

        # æž„å»ºæ€§èƒ½æ•°æ®
        performance_data = self._build_performance_data(node_timings, total_elapsed)

        # å°†æ€§èƒ½æ•°æ®æ·»åŠ åˆ°çŠ¶æ€ä¸­
        final_state['performance_metrics'] = performance_data

        # Store current state for reflection
        self.curr_state = final_state

        # Log state
        self._log_state(trade_date, final_state)

        # èŽ·å–æ¨¡åž‹ä¿¡æ¯
        model_info = ""
        try:
            if hasattr(self.deep_thinking_llm, 'model_name'):
                model_info = f"{self.deep_thinking_llm.__class__.__name__}:{self.deep_thinking_llm.model_name}"
            else:
                model_info = self.deep_thinking_llm.__class__.__name__
        except Exception:
            model_info = "Unknown"

        # å¤„ç†å†³ç­–å¹¶æ·»åŠ æ¨¡åž‹ä¿¡æ¯ï¼ˆå…¼å®¹æœªå¯ç”¨åŽç»­é˜¶æ®µï¼‰
        final_signal = (
            final_state.get("final_trade_decision")
            or final_state.get("investment_plan")
            or (final_state.get("risk_debate_state") or {}).get("judge_decision")
            or final_state.get("trader_investment_plan")
            or ""
        )
        if final_signal:
            decision = self.process_signal(final_signal, company_name)
        else:
            # åŽç»­é˜¶æ®µæœªå¼€å¯æ—¶çš„å…œåº•ç»“æž„ï¼Œé¿å…å‰ç«¯/æœåŠ¡ç«¯ KeyError
            # ðŸ”¥ ä¿®å¤ï¼šä½¿ç”¨ä¸Ž process_signal ä¸€è‡´çš„å­—æ®µå
            decision = {
                "action": "è§‚æœ›",
                "target_price": None,
                "confidence": 0,
                "risk_score": 0,
                "risk_level": "æœªçŸ¥",
                "reasoning": "æœªå¼€å¯æ·±åº¦å†³ç­–é˜¶æ®µï¼Œæœªç”Ÿæˆæœ€ç»ˆå†³ç­–",
                "reason": "æœªå¼€å¯æ·±åº¦å†³ç­–é˜¶æ®µï¼Œæœªç”Ÿæˆæœ€ç»ˆå†³ç­–",  # ä¿ç•™å…¼å®¹æ€§
            }
        decision["model_info"] = model_info

        # Return decision and processed signal
        return final_state, decision

    def _send_progress_update(self, chunk, progress_callback):
        """å‘é€è¿›åº¦æ›´æ–°åˆ°å›žè°ƒå‡½æ•°

        LangGraph stream è¿”å›žçš„ chunk æ ¼å¼ï¼š{node_name: {...}}
        èŠ‚ç‚¹åç§°ç¤ºä¾‹ï¼š
        - "Market Analyst", "Fundamentals Analyst", "News Analyst", "Social Analyst"
        - "tools_market", "tools_fundamentals", "tools_news", "tools_social"
        - "Msg Clear Market", "Msg Clear Fundamentals", etc.
        - "Bull Researcher", "Bear Researcher", "Research Manager"
        - "Trader"
        - "Risky Analyst", "Safe Analyst", "Neutral Analyst", "Risk Judge"
        """
        try:
            # ä»Žchunkä¸­æå–å½“å‰æ‰§è¡Œçš„èŠ‚ç‚¹ä¿¡æ¯
            if not isinstance(chunk, dict):
                return

            # èŽ·å–ç¬¬ä¸€ä¸ªéžç‰¹æ®Šé”®ä½œä¸ºèŠ‚ç‚¹å
            node_name = None
            for key in chunk.keys():
                if not key.startswith('__'):
                    node_name = key
                    break

            if not node_name:
                return

            logger.info(f"ðŸ” [Progress] èŠ‚ç‚¹åç§°: {node_name}")

            # æ£€æŸ¥æ˜¯å¦ä¸ºç»“æŸèŠ‚ç‚¹
            if '__end__' in chunk:
                logger.info(f"ðŸ“Š [Progress] æ£€æµ‹åˆ°__end__èŠ‚ç‚¹")
                progress_callback("ðŸ“Š ç”ŸæˆæŠ¥å‘Š")
                return

            # åŠ¨æ€æž„å»ºèŠ‚ç‚¹åç§°æ˜ å°„è¡¨ï¼ˆä»Žé…ç½®æ–‡ä»¶åŠ è½½ï¼‰
            from tradingagents.agents.analysts.dynamic_analyst import DynamicAnalystFactory
            node_mapping = DynamicAnalystFactory.build_node_mapping()

            # æŸ¥æ‰¾æ˜ å°„çš„æ¶ˆæ¯
            message = node_mapping.get(node_name)

            if message is None:
                # None è¡¨ç¤ºè·³è¿‡ï¼ˆå·¥å…·èŠ‚ç‚¹ã€æ¶ˆæ¯æ¸…ç†èŠ‚ç‚¹ï¼‰
                logger.debug(f"â­ï¸ [Progress] è·³è¿‡èŠ‚ç‚¹: {node_name}")
                return

            if message:
                # å‘é€è¿›åº¦æ›´æ–°
                logger.info(f"ðŸ“¤ [Progress] å‘é€è¿›åº¦æ›´æ–°: {message}")
                progress_callback(message)
            else:
                # æœªçŸ¥èŠ‚ç‚¹ï¼Œä½¿ç”¨èŠ‚ç‚¹åç§°
                logger.warning(f"âš ï¸ [Progress] æœªçŸ¥èŠ‚ç‚¹: {node_name}")
                progress_callback(f"ðŸ” {node_name}")

        except Exception as e:
            logger.error(f"âŒ è¿›åº¦æ›´æ–°å¤±è´¥: {e}", exc_info=True)

    def _build_performance_data(self, node_timings: Dict[str, float], total_elapsed: float) -> Dict[str, Any]:
        """æž„å»ºæ€§èƒ½æ•°æ®ç»“æž„

        Args:
            node_timings: æ¯ä¸ªèŠ‚ç‚¹çš„æ‰§è¡Œæ—¶é—´å­—å…¸
            total_elapsed: æ€»æ‰§è¡Œæ—¶é—´

        Returns:
            æ€§èƒ½æ•°æ®å­—å…¸
        """
        # èŠ‚ç‚¹åˆ†ç±»ï¼ˆæ³¨æ„ï¼šé£Žé™©ç®¡ç†èŠ‚ç‚¹è¦å…ˆäºŽåˆ†æžå¸ˆèŠ‚ç‚¹åˆ¤æ–­ï¼Œå› ä¸ºå®ƒä»¬ä¹ŸåŒ…å«'Analyst'ï¼‰
        analyst_nodes = {}
        tool_nodes = {}
        msg_clear_nodes = {}
        research_nodes = {}
        trader_nodes = {}
        risk_nodes = {}
        other_nodes = {}

        for node_name, elapsed in node_timings.items():
            # ä¼˜å…ˆåŒ¹é…é£Žé™©ç®¡ç†å›¢é˜Ÿï¼ˆå› ä¸ºå®ƒä»¬ä¹ŸåŒ…å«'Analyst'ï¼‰
            if 'Risky' in node_name or 'Safe' in node_name or 'Neutral' in node_name or 'Risk Judge' in node_name:
                risk_nodes[node_name] = elapsed
            # ç„¶åŽåŒ¹é…åˆ†æžå¸ˆå›¢é˜Ÿ
            elif 'Analyst' in node_name:
                analyst_nodes[node_name] = elapsed
            # å·¥å…·èŠ‚ç‚¹
            elif node_name.startswith('tools_'):
                tool_nodes[node_name] = elapsed
            # æ¶ˆæ¯æ¸…ç†èŠ‚ç‚¹
            elif node_name.startswith('Msg Clear'):
                msg_clear_nodes[node_name] = elapsed
            # ç ”ç©¶å›¢é˜Ÿ
            elif 'Researcher' in node_name or 'Research Manager' in node_name:
                research_nodes[node_name] = elapsed
            # äº¤æ˜“å›¢é˜Ÿ
            elif 'Trader' in node_name:
                trader_nodes[node_name] = elapsed
            # å…¶ä»–èŠ‚ç‚¹
            else:
                other_nodes[node_name] = elapsed

        # è®¡ç®—ç»Ÿè®¡æ•°æ®
        slowest_node = max(node_timings.items(), key=lambda x: x[1]) if node_timings else (None, 0)
        fastest_node = min(node_timings.items(), key=lambda x: x[1]) if node_timings else (None, 0)
        avg_time = sum(node_timings.values()) / len(node_timings) if node_timings else 0

        return {
            "total_time": round(total_elapsed, 2),
            "total_time_minutes": round(total_elapsed / 60, 2),
            "node_count": len(node_timings),
            "average_node_time": round(avg_time, 2),
            "slowest_node": {
                "name": slowest_node[0],
                "time": round(slowest_node[1], 2)
            } if slowest_node[0] else None,
            "fastest_node": {
                "name": fastest_node[0],
                "time": round(fastest_node[1], 2)
            } if fastest_node[0] else None,
            "node_timings": {k: round(v, 2) for k, v in node_timings.items()},
            "category_timings": {
                "analyst_team": {
                    "nodes": {k: round(v, 2) for k, v in analyst_nodes.items()},
                    "total": round(sum(analyst_nodes.values()), 2),
                    "percentage": round(sum(analyst_nodes.values()) / total_elapsed * 100, 1) if total_elapsed > 0 else 0
                },
                "tool_calls": {
                    "nodes": {k: round(v, 2) for k, v in tool_nodes.items()},
                    "total": round(sum(tool_nodes.values()), 2),
                    "percentage": round(sum(tool_nodes.values()) / total_elapsed * 100, 1) if total_elapsed > 0 else 0
                },
                "message_clearing": {
                    "nodes": {k: round(v, 2) for k, v in msg_clear_nodes.items()},
                    "total": round(sum(msg_clear_nodes.values()), 2),
                    "percentage": round(sum(msg_clear_nodes.values()) / total_elapsed * 100, 1) if total_elapsed > 0 else 0
                },
                "research_team": {
                    "nodes": {k: round(v, 2) for k, v in research_nodes.items()},
                    "total": round(sum(research_nodes.values()), 2),
                    "percentage": round(sum(research_nodes.values()) / total_elapsed * 100, 1) if total_elapsed > 0 else 0
                },
                "trader_team": {
                    "nodes": {k: round(v, 2) for k, v in trader_nodes.items()},
                    "total": round(sum(trader_nodes.values()), 2),
                    "percentage": round(sum(trader_nodes.values()) / total_elapsed * 100, 1) if total_elapsed > 0 else 0
                },
                "risk_management_team": {
                    "nodes": {k: round(v, 2) for k, v in risk_nodes.items()},
                    "total": round(sum(risk_nodes.values()), 2),
                    "percentage": round(sum(risk_nodes.values()) / total_elapsed * 100, 1) if total_elapsed > 0 else 0
                },
                "other": {
                    "nodes": {k: round(v, 2) for k, v in other_nodes.items()},
                    "total": round(sum(other_nodes.values()), 2),
                    "percentage": round(sum(other_nodes.values()) / total_elapsed * 100, 1) if total_elapsed > 0 else 0
                }
            },
            "llm_config": {
                "provider": self.config.get('llm_provider', 'unknown'),
                "deep_think_model": self.config.get('deep_think_llm', 'unknown'),
                "quick_think_model": self.config.get('quick_think_llm', 'unknown')
            }
        }

    def _print_timing_summary(self, node_timings: Dict[str, float], total_elapsed: float):
        """æ‰“å°è¯¦ç»†çš„æ—¶é—´ç»Ÿè®¡æŠ¥å‘Š

        Args:
            node_timings: æ¯ä¸ªèŠ‚ç‚¹çš„æ‰§è¡Œæ—¶é—´å­—å…¸
            total_elapsed: æ€»æ‰§è¡Œæ—¶é—´
        """
        logger.info("ðŸ” [_print_timing_summary] æ–¹æ³•è¢«è°ƒç”¨")
        logger.info("ðŸ” [_print_timing_summary] node_timings æ•°é‡: " + str(len(node_timings)))
        logger.info("ðŸ” [_print_timing_summary] total_elapsed: " + str(total_elapsed))

        logger.info("=" * 80)
        logger.info("â±ï¸  åˆ†æžæ€§èƒ½ç»Ÿè®¡æŠ¥å‘Š")
        logger.info("=" * 80)

        # èŠ‚ç‚¹åˆ†ç±»ï¼ˆæ³¨æ„ï¼šé£Žé™©ç®¡ç†èŠ‚ç‚¹è¦å…ˆäºŽåˆ†æžå¸ˆèŠ‚ç‚¹åˆ¤æ–­ï¼Œå› ä¸ºå®ƒä»¬ä¹ŸåŒ…å«'Analyst'ï¼‰
        analyst_nodes = []
        tool_nodes = []
        msg_clear_nodes = []
        research_nodes = []
        trader_nodes = []
        risk_nodes = []
        other_nodes = []

        for node_name, elapsed in node_timings.items():
            # ä¼˜å…ˆåŒ¹é…é£Žé™©ç®¡ç†å›¢é˜Ÿï¼ˆå› ä¸ºå®ƒä»¬ä¹ŸåŒ…å«'Analyst'ï¼‰
            if 'Risky' in node_name or 'Safe' in node_name or 'Neutral' in node_name or 'Risk Judge' in node_name:
                risk_nodes.append((node_name, elapsed))
            # ç„¶åŽåŒ¹é…åˆ†æžå¸ˆå›¢é˜Ÿ
            elif 'Analyst' in node_name:
                analyst_nodes.append((node_name, elapsed))
            # å·¥å…·èŠ‚ç‚¹
            elif node_name.startswith('tools_'):
                tool_nodes.append((node_name, elapsed))
            # æ¶ˆæ¯æ¸…ç†èŠ‚ç‚¹
            elif node_name.startswith('Msg Clear'):
                msg_clear_nodes.append((node_name, elapsed))
            # ç ”ç©¶å›¢é˜Ÿ
            elif 'Researcher' in node_name or 'Research Manager' in node_name:
                research_nodes.append((node_name, elapsed))
            # äº¤æ˜“å›¢é˜Ÿ
            elif 'Trader' in node_name:
                trader_nodes.append((node_name, elapsed))
            # å…¶ä»–èŠ‚ç‚¹
            else:
                other_nodes.append((node_name, elapsed))

        # æ‰“å°åˆ†ç±»ç»Ÿè®¡
        def print_category(title: str, nodes: List[Tuple[str, float]]):
            if not nodes:
                return
            logger.info(f"\nðŸ“Š {title}")
            logger.info("-" * 80)
            total_category_time = sum(t for _, t in nodes)
            for node_name, elapsed in sorted(nodes, key=lambda x: x[1], reverse=True):
                percentage = (elapsed / total_elapsed * 100) if total_elapsed > 0 else 0
                logger.info(f"  â€¢ {node_name:40s} {elapsed:8.2f}ç§’  ({percentage:5.1f}%)")
            logger.info(f"  {'å°è®¡':40s} {total_category_time:8.2f}ç§’  ({total_category_time/total_elapsed*100:5.1f}%)")

        print_category("åˆ†æžå¸ˆå›¢é˜Ÿ", analyst_nodes)
        print_category("å·¥å…·è°ƒç”¨", tool_nodes)
        print_category("æ¶ˆæ¯æ¸…ç†", msg_clear_nodes)
        print_category("ç ”ç©¶å›¢é˜Ÿ", research_nodes)
        print_category("äº¤æ˜“å›¢é˜Ÿ", trader_nodes)
        print_category("é£Žé™©ç®¡ç†å›¢é˜Ÿ", risk_nodes)
        print_category("å…¶ä»–èŠ‚ç‚¹", other_nodes)

        # æ‰“å°æ€»ä½“ç»Ÿè®¡
        logger.info("\n" + "=" * 80)
        logger.info(f"ðŸŽ¯ æ€»æ‰§è¡Œæ—¶é—´: {total_elapsed:.2f}ç§’ ({total_elapsed/60:.2f}åˆ†é’Ÿ)")
        logger.info(f"ðŸ“ˆ èŠ‚ç‚¹æ€»æ•°: {len(node_timings)}")
        if node_timings:
            avg_time = sum(node_timings.values()) / len(node_timings)
            logger.info(f"â±ï¸  å¹³å‡èŠ‚ç‚¹è€—æ—¶: {avg_time:.2f}ç§’")
            slowest_node = max(node_timings.items(), key=lambda x: x[1])
            logger.info(f"ðŸŒ æœ€æ…¢èŠ‚ç‚¹: {slowest_node[0]} ({slowest_node[1]:.2f}ç§’)")
            fastest_node = min(node_timings.items(), key=lambda x: x[1])
            logger.info(f"âš¡ æœ€å¿«èŠ‚ç‚¹: {fastest_node[0]} ({fastest_node[1]:.2f}ç§’)")

        # æ‰“å°LLMé…ç½®ä¿¡æ¯
        logger.info(f"\nðŸ¤– LLMé…ç½®:")
        logger.info(f"  â€¢ æä¾›å•†: {self.config.get('llm_provider', 'unknown')}")
        logger.info(f"  â€¢ æ·±åº¦æ€è€ƒæ¨¡åž‹: {self.config.get('deep_think_llm', 'unknown')}")
        logger.info(f"  â€¢ å¿«é€Ÿæ€è€ƒæ¨¡åž‹: {self.config.get('quick_think_llm', 'unknown')}")
        logger.info("=" * 80)

    def _log_state(self, trade_date, final_state):
        """Log the final state to a JSON file."""
        inv_state = final_state.get("investment_debate_state") or {}
        risk_state = final_state.get("risk_debate_state") or {}

        def _safe(d, key, default=""):
            return d.get(key, default) if isinstance(d, dict) else default

        # ðŸ”¥ åŠ¨æ€å‘çŽ°æ‰€æœ‰ *_report å­—æ®µï¼Œè‡ªåŠ¨æ”¯æŒæ–°æ·»åŠ çš„åˆ†æžå¸ˆæŠ¥å‘Š
        all_reports = {}
        for key in final_state.keys():
            if key.endswith("_report"):
                all_reports[key] = final_state.get(key, "")
        
        self.log_states_dict[str(trade_date)] = {
            "company_of_interest": final_state.get("company_of_interest", ""),
            "trade_date": final_state.get("trade_date", ""),
            **all_reports,  # ðŸ”¥ åŠ¨æ€åŒ…å«æ‰€æœ‰æŠ¥å‘Š
            "investment_debate_state": {
                "bull_history": _safe(inv_state, "bull_history"),
                "bear_history": _safe(inv_state, "bear_history"),
                "history": _safe(inv_state, "history"),
                "current_response": _safe(inv_state, "current_response"),
                "judge_decision": _safe(inv_state, "judge_decision"),
            },
            "trader_investment_decision": final_state.get(
                "trader_investment_plan", ""
            ),
            "risk_debate_state": {
                "risky_history": _safe(risk_state, "risky_history"),
                "safe_history": _safe(risk_state, "safe_history"),
                "neutral_history": _safe(risk_state, "neutral_history"),
                "history": _safe(risk_state, "history"),
                "judge_decision": _safe(risk_state, "judge_decision"),
            },
            "investment_plan": final_state.get("investment_plan", ""),
            "final_trade_decision": final_state.get("final_trade_decision", ""),
        }

        # Save to file
        base_dir = get_eval_results_dir()
        directory = base_dir / self.ticker / "TradingAgentsStrategy_logs"
        directory.mkdir(parents=True, exist_ok=True)

        log_file = directory / "full_states_log.json"
        with log_file.open("w") as f:
            json.dump(self.log_states_dict, f, indent=4)

    def reflect_and_remember(self, returns_losses):
        """Reflect on decisions and update memory based on returns."""
        if not self.curr_state:
            return

        inv_state = self.curr_state.get("investment_debate_state") or {}
        risk_state = self.curr_state.get("risk_debate_state") or {}

        # ä»…åœ¨å¯¹åº”é˜¶æ®µå‚ä¸Žæ—¶æ‰å†™å…¥è®°å¿†ï¼Œé¿å…ç¼ºå¤±å­—æ®µæŠ¥é”™
        if inv_state:
            self.reflector.reflect_bull_researcher(
                self.curr_state, returns_losses, self.bull_memory
            )
            self.reflector.reflect_bear_researcher(
                self.curr_state, returns_losses, self.bear_memory
            )
            self.reflector.reflect_invest_judge(
                self.curr_state, returns_losses, self.invest_judge_memory
            )

        if self.curr_state.get("trader_investment_plan"):
            self.reflector.reflect_trader(
                self.curr_state, returns_losses, self.trader_memory
            )

        if risk_state:
            self.reflector.reflect_risk_manager(
                self.curr_state, returns_losses, self.risk_manager_memory
            )

    def process_signal(self, full_signal, stock_symbol=None):
        """Process a signal to extract the core decision."""
        return self.signal_processor.process_signal(full_signal, stock_symbol)
